\documentclass[11pt]{article}

\usepackage{preamble}
\input{math_commands.tex}

\title{Notes of Math 733: Probability Theory\\ Instructor: Timo Seppäläinen}
\author{YI WEI}
\date{Sep 2024}


\begin{document}

\maketitle
\tableofcontents



\section{Probability Space}
\DATE{Sep 5,2024}

Setup (Undergraduate level):
\begin{align*}
    &\Omega \text{ sample space: set of all the individual outcomes} \\
    &\mathcal{F}\text{ event space: appropriate collection of subsets of } \Omega\\
    &P: \text{a function on a subsets of } \Omega, P(A)= \text{ the probability of the set (event) }A 
\end{align*}

\begin{axiom}
    \begin{align*}
        P(\bigcup_k A_k) = \sum_{k} P(A_k) \quad \text{whenever } {A_k} \text{ is a pairwise disjoint sequence of events} 
    \end{align*}
\end{axiom}

\begin{example}
    \, 

    \begin{enumerate}
        \item roll a dice: $ \Omega = \{1,2,3,4,5,6 \}$, 
        $\mathcal{F} = \mathcal{P}(\Omega) = \text{ power set of }
        \; \Omega = \text{ collection of all subset of} \; \Omega$
        \item \# of customers to a service station in some fixed time interval 
                \begin{align*}
                    \Omega &= \mathbb{Z}_{\ge 0} \\
                    \mathcal{F} &= \mathcal{P}(\Omega)\\
                    P(k) &= e^{-\lambda }\frac{\lambda^k}{k!} \quad \text{for }k \in \Omega\\
                    P(A) &= \sum_{k \in A} e^{-\lambda }\frac{\lambda^k}{k!} \quad \text{for } A \subseteq \Omega
                \end{align*}
        \item Choose uniformly random real number from $[0,1]$ \\
                $P(x) = 0 \quad \forall \, x \in [0,1]$ \\
                if $0 \le a < b \le 1$:
                \begin{align*}
                    P([a,b]) = b-a
                \end{align*}
        \item \label{example:1.1.4} 
                Flip a fair coin for infinitly many times, $0 = \text{heads}, \, 1=\text{tails}$:
                \begin{align*}
                    &\Omega = \{0,1\}^{\mathbb{Z}_{\ge 0}} \\
                    &P\{w: x_1 = a_1, x_2=a_2, \ldots , x_n = a_n\} = 2^{-n} \label{*} \tag{*}
                \end{align*}
                From this: $P\{w\} = 0 \quad \forall w \in \Omega$
                \begin{exercise}
                    how to prove $\Omega$ is uncountable: diagonal principle
                \end{exercise}
    \end{enumerate}
\end{example}

\begin{definition}
    Let $X$ be a space. A $\sigma$-algebra on $X$ is a collection $\mathcal{A}$ of subsets of $X$ that
    satisfies these properties:
    \begin{enumerate}
        \item $\emptyset \in \mathcal{A}$
        \item $A \in \mathcal{A} \Longrightarrow  A^C \in \mathcal{A}$
        \item $\{A_k \}_{k=1}^{\infty} \Longrightarrow \bigcup_{k=1}^{\infty}A_k \in \mathcal{A}$ 
    \end{enumerate}
    And we call $(X, \mathcal{A})$ is a $\underline{\text{measurable space}}$.
\end{definition}

\begin{definition}
    Given $(X, \mathcal{A})$
    A measure is a function $u: \mathcal{A} \to [0, \infty]$ such that:
    \begin{enumerate}
        \item $P(\emptyset) = 0$
        \item $u( \bigcup_{k}A_k) = \sum_{k=1}u(A_k)$ for a pairwise disjoint sequence $\{A_k\}_k \subseteq \mathcal{A}$
    \end{enumerate}
    $(X, \mathcal{A}, u)$ is a $\underline{\text{measure space}}$.
\end{definition}

\begin{definition}
    If $X$ is a metric space, its $\underline{\text{Borel } \sigma\text{-algebra}} \,\, \mathcal{B}_{X}$ is by
    definition the smallest $\sigma$-algebra containing all the OPEN subsets of $X$.
\end{definition}

\begin{definition}
    $\underline{\text{Lebesgue measure}} \; m$ on $\mathbb{R}^d$ is the measure that satisfies 
    \begin{align*}
        m\Big( \prod_{i=1}^{d} [a_i,b_i]\Big) = \prod_{i=1}^{d} (b_i-a_i)
    \end{align*}
\end{definition}

\begin{definition}
    A $\underline{\text{probability space}} \; (\Omega, \mathcal{F}, P)$ is a measure space such that $P(\Omega) = 1$.
\end{definition}

\begin{example}
    Example of product $\sigma$-algebra from example 1.1.~\ref{example:1.1.4}: \\
    $\mathcal{F}$ = product $\sigma$-algebra $=$ samllest $\sigma$ -algebra that contains all sets of the type
    \begin{align*}
        \{w: x_1 = a_1, \ldots , x_{n} = a_{n}\} \quad , n \in \mathbb{Z}_{>0}, a_{1}, \ldots ,a_n \in \{0,1 \}.
    \end{align*}
    $P$ obtained from Eq.~\ref{*}
\end{example}

\begin{definition}
    Let $(X, \mathcal{A}), \, (Y, \mathcal{B})$ be measurable space, and $f: X \to Y$ be a function.\\
    We say $f$ is a $\underline{\text{measurable function}}$ if:
    \begin{align*}
        f^{-1}(B) = \{x \in X: f(x) \in \mathcal{B} \} \subseteq \mathcal{A}, \quad \forall B \in \mathcal{B}
    \end{align*}
    A $\underline{\text{random variable}} \; X$ is a measurable function:
    \begin{align*}
        X: (\Omega, \mathcal{F}) \to (\mathbb{R}, \mathcal{B}_{\mathbb{R}})
    \end{align*}
\end{definition}

\begin{example}
    flip of a fair coin $\Omega = \{ w = (x_1,x_2): x_1,x_2 \in \{0,1\}\}$, $0 = \text{heads}, \, 1=\text{tails}$:
    \begin{align*}
        X_1(w) &= x_1  \quad \text{outcome of the first flip}\\
        X_2(w) &= x_2  \quad \text{outcome of the second flip}
    \end{align*}
    We define $Y(w) = X_1(w) + X_2(w)$ = \# of tails in the two flips \\
    The information contained in $Y(w)$ is represented by $\sigma$-algebra generated by $Y$ defined as follows:
    \begin{align*}
        \sigma(Y) &= \{\{Y \in B\}: B \in \mathcal{B}_{\mathbb{R}}\}\\
        &= \Big\{\emptyset, \Omega, \{(0,0)\}, \{(0,1),(1,0) \} , \{(1,1)\} \text{ and the unions of these sets}\Big\} \subsetneq \mathcal{F}
    \end{align*}
\end{example}


\DATE{Sep 10,2024}
\begin{enumerate}
    \item push-forward: $(X, \mathcal{A}, \mu)$ is a measure space, 
    and $(Y, \mathcal{B})$ is a measurable space. And there is a $f: X \to Y$.
    The push-forward of $\mu$ is the measure $v$ on $(Y, \mathcal{B})$ defined by $v(\mathcal{B}) =
    u(f^{-1}(\mathcal{B}))$
    \begin{exercise}
        Check $v$ is a measure.
    \end{exercise}

    \item Absolute continuity: Let $\mu, \lambda$ be measures on $(X, \mathcal{A})$. Then $\mu$ is absolute
    continuous w.r.t $\lambda$ if $\lambda(A) = 0 \Longrightarrow \mu(A) = 0 \quad \forall  A \in \mathcal{A}$.
    \remark $\mu \ll \lambda$. If $\mu \ll \lambda$, then there exists a measurable function 
    $f: X \to \mathbb{R}_{\ge 0}$ s.t. 
    \begin{align*}
        \mu(A) = \int_{A} f \, d \lambda \qquad \forall A \in \mathcal{A}
    \end{align*}
    This is called Radom-Nikodym derivative $f(x) = \frac{d \mu}{d \lambda}(x)$
\end{enumerate}

\begin{definition}
    Let $X: (\Omega, \mathcal{F}, P) \to (\mathbb{R}, \mathcal{B}_{\mathbb{R}})$ be a random variable.
    The $\underline{\text{distribution}}$ of $X$ is the $\mu = P \circ X^{-1}$, i.e.,
    \begin{align*}
        \mu (B) = P\{w \in \Omega: X(w) \in B\} \qquad \text{for } B \in \mathcal{B}
    \end{align*}
    In short: $P\{X \in B\} = P(X \in B)$
\end{definition}

\begin{definition}
    The CDF of $X$ is the function $F$ on $\mathbb{R}$ defined by 
    \begin{align*}
        F(x) = P(X \le x) = \mu(-\infty,x]
    \end{align*}
\end{definition}
\begin{definition}
    If $\mu \ll $ Lebegue measure, then $X$ has a density function $f$ which satisfies 
    \begin{align*}
        P(a < X \le b) = \int _{a}^{b} f(x) \, dx = \mu(a,b] = F(b) - F(a)
    \end{align*}
\end{definition}

\begin{remark}
    A $\underline{\text{discrete random variable}}$ has at most countably many values, and since individual pts have
    positive probability
    \begin{align*}
        \mu \{k\} = P(X=k) > 0 = leb\{x\}
    \end{align*}
    Then we know $\mu \ll $Leb fails and X has no density function.
\end{remark}


\begin{definition}
    The $\underline{\text{expectation}}$ of a r.v. $X$ is defined by 
    \begin{align*}
        EX = \int_{\Omega} X \, dP
    \end{align*}
    \begin{remark}
        Abstract Lebesgue integral on $(\Omega, \mathcal{F}, P)$
    \end{remark}
\end{definition}

% \begin{definition}
%     If $A \in \mathcal{F}$ is an event, its indicator r.v. is 
%     \begin{align*}
%         \1 _{A}(w) = 
%         \begin{cases}
%             \item 0, w \not A 
%             \item 1, w \in A
%         \end{cases}
%     \end{align*}
% \end{definition}
\begin{definition}
    If $A \in \mathcal{F}$ is an event, its indicator random variable is 
    \[
    \1 _{A}(w) = 
    \begin{cases} 
        1, & \text{if } w \in A, \\
        0, & \text{if } w \notin A.
    \end{cases}
    \]
\end{definition}

We know 
\begin{align*}
    E[\1_{A}] &= 0 \cdot P\{\1_{A} = 0\} + 1 \cdot  P\{\1_{A} = 1\}\\
    &= P(A)
\end{align*}

\begin{example}
    \item $X \sim Poisson(\lambda) \Longrightarrow E[g(X)] = \sum_{k=0}^{\infty} g(k) 
    \frac{e^{-\lambda}\lambda^k}{k!}$
    \item $X \sim  Exp(\lambda) \Longrightarrow E[g(X)] = \int_{0}^{\infty} g(x) \lambda 
    e^{-\lambda } \mathrm{d}x$
\end{example}


\begin{theorem}
    \,

    $\underline{\text{Key result}}$: 
    $$E[f(X)] := \int_{\Omega}f(X)  \mathrm{d}P = \int_{\mathbb{R}}f \, d \mu$$

    Here: $X$ is a r.v. on $(\Omega, \mathcal{F}, P)$, $\mu = P \circ X^{-1}=$ distribution of $X$ ,
    $f: \mathbb{R} \to \mathbb{R}$ is a Borel function $f(X(w)) = (f \circ X)(w)$
\end{theorem}
\begin{proof}
    \,

    \begin{enumerate}
        \item $f = \1_{B}, B \in \mathcal{B}_{\mathbb{R}}$.
        \begin{remark}
            Notation: $\int_{\Omega} \1_{B}(X(w))  P(\mathrm{d}w)$ (same as $dP(w)$)
            \begin{align*}
                \int_{\Omega} \1_{B}(X(w))  P(\mathrm{d}w) &= \int_{\Omega}\1_{X^{-1}(\mathcal{B})}(w)\mathrm{d}x\\
                &= P(X^{-1}(B)) = \mu(B) = \int_{\mathbb{R}} \1_{B} d \mu
            \end{align*}
        \end{remark}
        \item $f = \sum_{i=1}^{n} a_{i} \1_{B_{i}}$, $a_1, \ldots ,a_n \in \mathbb{R}$, $B_1, \ldots ,B_n \in \mathcal{B}_{\mathbb{R}}$
        \begin{align*}
            \int_{\Omega} \sum_{i=1}^{n}a_{i}\1_{B_i}(X) \, dP 
            &= \sum_{i=1}^{n}a_{i}  \int_{\Omega} \1_{B_i}(X) \, dP\\
            &= \sum_{i=1}^{n}a_i \int_{\mathbb{R}} \1_{B_i}\, d \mu \\
            &= \int_{R} \sum_{i=1}^{n}a_i \1_{B_i} \, d \mu
        \end{align*}
        \item $f \ge 0, \exists $ simple function $0 \le f_n $
        \begin{align*}
            \int _{\Omega} f(X) \, dP &= \lim_{n \to \infty} \int_{\Omega} f_n(X) \, dP \qquad (M.C.T.)\\
            &= \lim_{n \to \infty} \int_{\mathbb{R}} f_n \, d \mu \\
            &= \int_{\mathbb{R}} f \, d \mu
        \end{align*}
        \begin{remark}
            $f_n(x) = \sum_{k=0}^{n(2^n - 1)} \frac{k}{2^n} \1 \{\frac{k}{2^n} \le f(x) < \frac{k+1}{2^n}\}
            + n \1 \{ f(x) > n\}$
        \end{remark}
        \item For general $f: \mathbb{R} \to \mathbb{R} = f^{+} - f^{-}$ Borel function where $f^{+}, f^{-} \ge 0$
        \begin{align*}
            \int _{\Omega} f(X) \, dP &= \int _{\Omega} f^{+}(X) \, dP - \int _{\Omega} f^{-}(X) \, dP\\
            &= \int _{\mathbb{R}} f^{+} \, d \mu - \int _{\mathbb{R}} f^{-} \, d \mu \\
            &= \int_{\mathbb{R}} f \, d \mu
        \end{align*}
    \end{enumerate}
\end{proof}

\begin{example}
    \begin{enumerate}
        \item $X \sim Possion(\lambda)$, $\mu = $ distribution of $X$.
        We know $\mu(B) = \sum_{k: k\in B} e^{-\lambda} \frac{\lambda^k}{k!} \Longrightarrow
        \mu(\mathbb{R} \setminus{\mathbb{Z}_{\ge 0}})= 0$. Then we have:
        \begin{align*}
            E[e^{-tX}] &= \int_{\mathbb{R}} e^{-tx} \mu(dx)\\
            &=\int_{\mathbb{Z}_{\ge 0}} e^{-tx} \mu(dx) \\
            &= \sum_{k \in \mathbb{Z}_{k\ge 0}} \int_{\{k\}} e^{-tx} \, \mu(dx)\\
            &= \sum_{k \ge 0} e^{-tk} e^{-\lambda} \frac{\lambda^k}{k!}\\
            &= e^{-\lambda} e^{\lambda e^{-t}} \\
            &= e^{\lambda(e^{-t}-1)}
        \end{align*}
        \item $X \sim Exp(\lambda)$
        \begin{align*}
            E[e^{-tX}] &= \int_{\mathbb{R}} e^{-tx} \, \mu(dx) = \int_{[0, \infty)} e^{-tx} \lambda e^{-\lambda x} \, dx \\
            &= \lim_{M \to \infty} \int_{[0,M]} \lambda e^{-(t+\lambda)x} \, dx = \lim_{M \to \infty}
            R \int_{0}^{M} \lambda e^{-(t+\lambda)x} \mathrm{d}x\\
            &= \lim_{M \to \infty} (-\frac{\lambda}{t+\lambda}) e^{-(t+\lambda)x} |^M_{0} \\
            &= \lim_{M \to \infty} \Big( (-\frac{\lambda}{t+\lambda}) e^{-(t+\lambda)M} + \frac{\lambda}{t+\lambda} \Big)\\
            &= \frac{\lambda}{t+\lambda}
        \end{align*}
    \end{enumerate}
\end{example}


\newpage
\section{Laws of Large Numbers}

\DATE{Sep 12, 2024}
\subsection{Independence}

Perhaps you recall this: events $A$ and $B$ are independent if $P(A \cap B) = P(A)P(B)$

\begin{definition}
    Let $\Omega, \mathcal{F}, P$ be a probability space. 

    Let $\mathcal{F}_1, \ldots ,\mathcal{F}_n$ be sub-$\sigma$-algebra of $\mathcal{F}$. (Means
    each $\mathcal{F}_i$ is a $\sigma$-algebra and $\mathcal{F}_i \subseteq \mathcal{F}$)

    Then we say $\mathcal{F}_1, \ldots ,\mathcal{F}_n$ are independent if $\forall A_1 \in \mathcal{F}_1,
    A_2 \in \mathcal{F}_2, \ldots ,A_n \in \mathcal{F}_n$, then
    \begin{align*}
        P(\bigcap_{i=1}^{n} A_i) = \prod_{i=1}^{n}P(A_i) 
    \end{align*}
    Now r.v.'s $X_1, \ldots ,X_n$ on $\Omega$ are independent if the $\sigma$-algebra 
    $\sigma(X_1), \ldots ,\sigma(X_n)$ are independent. Equivalently, $\forall $ measurable 
    sets in the range space,
    \begin{align*}
       P(\bigcap_{i=1}^{n}\{X_{i} \in B_i\}) = \prod_{i=1}^{n} P\{X_{i} \in B_i\} 
    \end{align*}
\end{definition}

Events $A_1, \ldots ,A_n$ are independent if the r.v.'s $\1_{A_1}, \ldots ,\1_{A_n}$ are independent.

And arbitrary collection $\{\mathcal{F}_{\beta}: \beta \in \mathcal{J}\}$ of sub-$\sigma$-algebra is independent if
$\forall \text{distinct } \beta_1, \ldots ,\beta_{n} \in \mathcal{J}$, $\mathcal{F}_{\beta_{1}}, \ldots ,\mathcal{F}_{\beta_{n}}$
are independent.

\begin{claim}
    $\underline{\text{Fact:}}$ $X_1, \ldots ,X_n$ are independent, then so are 
    $f_1(X_1), \ldots ,f_n(X_n)$
\end{claim}

\begin{remark}
    Why product?

    $X,Y$ discrete r.v.'s. We're interested in the event $\{X=k\}$. Suppose we learn that $Y=m$.
    We replace P with $P(\cdot , Y=m)$ defined by $P(A| Y=m) = \frac{P(A \bigcap \{Y=m\})}{P(Y=m)}$

    When is $P(X=k) = P(X=k | Y=m)$?

    \begin{align*}
        P(X=k) &= P(X=k | Y=m)\\
        \iff P(X=k)P(Y=m) &= P(X=k,Y=m)
    \end{align*}
\end{remark}

We need some notions/tool to check easily if two r.v.'s are independent.
\begin{enumerate}
    \item Develop a simpler criterion for checking independence of a given collection of r.v.'s.
    \item To construct a probability space with desired independent r.v.'s.
\end{enumerate}

\begin{example}
    Let $X_1, X_2,X_3$ be independent $Bernolli(p)$ r.v.'s.
    \begin{align*}
        P(X_{i} = 1) = p = 1-P(X_{i} = 0)
    \end{align*}
    Consider the following events:
    \begin{equation*}
        \begin{cases} 
        \{X_1 + X_2 = 1\}\\ 
        \{X_2+X_3 = 1 \}  
        \end{cases}
    \end{equation*}
    Firstly we have:
    \begin{align*}
        P(X_1 + X_2 = 1) = P(01) + P(10) = 2p(1-p) = P(X_2+X_3 = 1)
    \end{align*}
    And we have:
    \begin{align*}
        P(X_1+X_2=1, X_2+X_3=1) = P(101) + P(010) = p^{2}(1-p) + p(1-p)^2 = p(1-p)
    \end{align*}
    If the two events and independent, we have:
    \begin{align*}
        P(X_1+X_2=1, X_2+X_3=1) &= P(X_1 + X_2 = 1) \cdot P(X_2+X_3 = 1)\\
        \iff p(1-p) &= 4p^{2}(1-p)^{2} \\
        \iff p(1-p) &= \frac{1}{4}, \; p =0,\; \text{or} \; p=1 \\
        \iff p = \frac{1}{2},\; &0,\; \text{or} \; 1
    \end{align*}
\end{example}


\begin{theorem}
    \label{theorem:2.2}
    Let $\mathcal{A}_{1}, \ldots ,\mathcal{A}_{n}$ be subcollection of $\mathcal{F}$,
    Assume that each $\mathcal{A}_{i}$ is closed under intersection, which means 
    $(A,B \in \mathcal{A}_{i} \Longrightarrow A \bigcap B \in \mathcal{A}_{i})$ and $\Omega \in \mathcal{A}_{i}$.
    Assume that the probability $P(\bigcap_{i=1}^{n} A_{i}) = \prod_{i=1}^{n}P(A_i) \quad
    \forall A_1 \in \mathcal{A}_{1}, \ldots ,A_n \in \mathcal{A}_{n}$. Then the $\sigma$-algebra 
    $\sigma(\mathcal{A}_{1}), \ldots ,\sigma(\mathcal{A}_{n})$ are independent.
\end{theorem}
\begin{example}
    Collection of sets which can generate Borel-algebra:

    $A_i = \{(a,b): -\infty < a < b <\infty\}$, then $\sigma(A_{i}) = \mathcal{B}_{\mathbb{R}}$.

    Or you can take $(-\infty, b]$ ......
\end{example}

\vspace*{5mm}
The tool for proving the theorem: $\underline{\text{Dynkin's} \;\pi-\lambda \text{ theorem}}$.

\begin{definition}
    Let $\mathcal{A}$ be a collection of subset of $\Omega$
    \begin{enumerate}
        \item $\mathcal{A}$ is a $\pi$-system if it is closed under intersections.
        \item $\mathcal{A}$ is a $\lambda$-system if it has the following three properties:
        \begin{enumerate}
            \item $\Omega \in \mathcal{A}$
            \item $\forall A,B \in \mathcal{A}$ and $A \subseteq B \Longrightarrow B \setminus A \in \mathcal{A}$
            \item If $A_1 \subseteq A_2 \cdots \subseteq A_n \subseteq 
            \cdots$ and each $A_{i} \in \mathcal{A}$, then
            $\bigcup_{i=1}^{\infty} A_{i} \in \mathcal{A}$
        \end{enumerate}
    \end{enumerate}
\end{definition}

\begin{theorem}
    \label{theorem:2.3}
    Suppose $\mathcal{P}$ is a $\pi$-system, $\mathcal{L}$ is a $\lambda$-system and $\mathcal{P} \subseteq \mathcal{L}$,
    then $\sigma(\mathcal{P}) \subseteq \mathcal{L}$
\end{theorem}
We use theorem~\ref{theorem:2.3} to prove theorem~\ref{theorem:2.2}.

\vspace*{7mm}
\begin{proof}[Proof of theorem 2.2:]
    \,
    
    Fix $A_2 \in \mathcal{A}_{2}, \ldots ,A_n \in \mathcal{A}_{n}$, set 
    $\mathcal{F} = A_2 \bigcap \cdots \bigcap A_n$
    \begin{align*}
        \mathcal{L} = \{ A \in \mathcal{F}: P(A \bigcap F) = P(A)P(F) \}
    \end{align*}
    \begin{claim}
        $\mathcal{A}_1 \subseteq \mathcal{L}$.
        \begin{proof}[Proof of Claim 2.4]
            \,

            Check that $P(F) = \prod_{i=2}^{n}P(A_{i}) $

            Take $A_1 = \Omega$

            Let $A_1 \in \mathcal{A}_{1}$. $P(A_{1} \bigcap F) = P(\bigcap_{i=1}^{n}A_{i})
            = \prod_{i=1}^{n}P(A_{i}) = P(A_{i})P(F) $
        \end{proof}
    \end{claim}
    \begin{claim}
        $\mathcal{L}$ is a $\lambda$-system.
        \begin{proof}[Proof of Claim 2.5]
            \,

            \begin{enumerate}
                \item $\Omega \in \mathcal{A}_{1} \subseteq \mathcal{L}$
                \item Let $A,B \in \mathcal{L}, A \subseteq B$. We want $B\setminus A \in \mathcal{L}$.
                \begin{align*}
                    P\Big((B\setminus A) \bigcap F\Big) = P\Big((B \bigcap F)\setminus(A \bigcap F)\Big)
                    = P(B \bigcap F) - P(A \bigcap F)
                \end{align*}
                \item Let $\mathcal{L} \ni A_{i} \nearrow A.$. We want: $A \in \mathcal{L}$
                \begin{align*}
                    P(A\bigcap F) = \lim_{n \to \infty}P(A_n \bigcap F) \qquad \text{because }
                    A_n \bigcap F \nearrow A\bigcap F
                \end{align*}
            \end{enumerate}
            We've checked that $\mathcal{L}$ is a $\lambda$-system. So $\sigma(A_1) \subseteq \mathcal{L}$

        \end{proof}
    \end{claim}
    We continue the proof of theorem~\ref{theorem:2.2}:

    Then $P(\bigcap _{i=1}^{n} A_i) = \prod_{i=1}^{n} P(A_{i}) \qquad \forall 
     A_1 \in \sigma(\mathcal{A}_1), A_2 \in \mathcal{A}_{2}, \ldots ,A_n \in \mathcal{A}_{n}$

     We can use the same argument to upgrade each $\mathcal{A}_i$ in turn to $\sigma(\mathcal{A}_{i})$.
     At the end we have the product properties for all members of $\sigma(\mathcal{A}_{1}), \ldots ,
     \sigma(\mathcal{A}_{n})$
\end{proof}

\begin{corollary}
    $\mathbb{R}$-valued r.v.'s $X_1, \ldots ,X_n$ are independent iff 
    \begin{align*}
        P(\bigcap _{i=1}^{n}\{ X_{i} \le s_i \}) = \prod_{i=1}^{n}P\{ X_{i} \le s_i \} 
    \end{align*}
\end{corollary}

\DATE{Sep 17, 2024}

Today:
\begin{equation*}
    \text{Independent r.v's}
    \begin{cases} 
    \text{product measure}\\ 
    \text{convolutions}   
    \end{cases}
\end{equation*}

\subsubsection{product measures}
\begin{definition}
    Suppose $(X_1, \mathcal{A}_1, \mu_1), \ldots ,(X_n,\mathcal{A}_n,\mu_n)$ are $\sigma$-finite measure spaces.
    The $\underline{\text{product}}$ measure space $(X,\mathcal{A},\mu)$ is defined as follows:
    \begin{align*}
        &X = \prod_{i=1}^{n} X_{i} = \text{ the Cartesian product, } \mathcal{A} = \text{ product }\sigma-algebra
        = \otimes_{i=1}^{n}\mathcal{A}_i = \sigma \{ A_1\times \cdot \times A_n: A_i \in \mathcal{A}_i \}\\
        &\mu = \text{product measure} = \otimes_{i=1}^{n}\mu_i = \text{ by def the unique measure }\mu \text{ on }
        \mathcal{A} \text{ such that }\\
        &\hspace*{15mm} \mu(A_1 \times \cdot  \times A_n) = \prod_{i=1}^{n}\mu_i(A_i) 
        \quad \forall A_1 \in \mathcal{A}_1, \ldots ,
        A_n \in \mathcal{A}_n 
    \end{align*}
\end{definition}

\begin{theorem}[Tonelli-Fubini Theorem]
    \,

    $(n=2)$: $\int_{X \times Y} f(x,y) \mu \otimes v (dx,dy) = \int_{Y}[\int_{X}f(x,y)\, \mu(dx)]v(dy)$
\end{theorem}

Suppose each $X_{i}$ is a metric space w.r.t $\mathcal{B}_{X_{i}}$; also $X$ is a 
metric space w.r.t $\mathcal{B}_{X}$. Relationship of $\otimes_{i=1}^{n}\mathcal{B}_{X_{i}}$ $\&$ $\mathcal{B}_{X}$.

Take $n=2$: $\mathcal{B}_{X} \otimes \mathcal{B}_{Y} = \sigma \{ A \times B: A \in \mathcal{B}_{X},
B \in \mathcal{B}_{Y} \} = \sigma\{\underbrace{A \times B}_{\text{open set in } X \times Y}:A
 \subseteq X \text{ open, }B \subseteq Y \text{ open } \}$ 

\begin{definition}
    Separable metric space has a countable dense subset.
\end{definition}

\begin{example}
    \,

    \begin{enumerate}
        \item $\mathbb{R}^{d}$
        \item $C[0,1]$
        \item $C([0,\infty])$: here the metric $d(f,g) = \sup_{0 \le x < \infty}|f(x) - g(x)|$ makes not separable!\\
        But $d(f,g) = \sum_{n=1}^{\infty}2^{-n}(\sup_{0\le x\le n}|f(x) - g(x)| \land 1)$
    \end{enumerate}
\end{example}

\begin{theorem}[Proposition 1.5 in Folland]
    \,

    $\underline{\text{Fact}}$: If $X,Y$ are separable metric spaces, then $\mathcal{B}_{X} \otimes \mathcal{B}_{Y}
    = \mathcal{B}_{X\times Y}$
\end{theorem}

\begin{remark}
    reference: Richard M. Dudley: Real Analysis and Probability, Prop 4.1.7
\end{remark}


\begin{definition}
    Suppose $X_1, \ldots ,X_n$ are r.v.'s on $(\Omega, \mathcal{F}, P)$. Let $\mu_i(B) = P(X_{i} \in B),
\, B \in \mathcal{B}_{\mathbb{R}}$ be the distribution (marginal distribution of $X_{i}$) of $X_{i}$

$ X = (X_1, \ldots ,X_n)$ is an $\mathbb{R}^{n}$-valued random variable and its distribution (joint distribution
of $X_1, \ldots ,X_n$)
is a probability measure $\mu$
on $\mathbb{R}^{n}$.
\end{definition}


\begin{theorem}
    $X_1, \ldots ,X_n \text{ independet }\iff \mu = \otimes_{i=1}^{n}\mu_i$
\end{theorem}
\begin{proof}
    \,

    \begin{enumerate}
        \item $\Longrightarrow$:
        Let $A_1 \times \cdots \times A_n \in \mathcal{B}_{\mathbb{R}^{n}}$.
        \begin{align*}
            \mu(A_1 \times \cdots \times A_n) = P\{ (X_1, \ldots ,X_n) \in A_1 \times \cdots\times A_n \}\\
            = P\{ X_1 \in A_1, \ldots ,X_n \in A_n \} 
                = \prod_{i=1}^{n}P(X_{i} \in A_i) 
                = \prod_{i=1}^{n}\mu_i(A_i). \\
            \pi-\lambda \text{ thm} \Longrightarrow \mu = \otimes_{i=1}^{n}\mu_i
        \end{align*}
        \item $\Longrightarrow$ Similar
    \end{enumerate}
\end{proof}

\begin{corollary}
    If $E|f_i(X_{i})| < \infty$ for $i=1, \ldots n$, $X_1, \ldots ,X_n$ independent, then
    \begin{align*}
        E\Big[\prod_{i=1}^{n}f_i(x_i)  \Big] = \prod_{i=1}^{n}E\Big[f_i(X_{i})\Big] 
    \end{align*}
\end{corollary}
\begin{proof}
    Note that when $X_1, \ldots ,X_n$ are independent, then $f(X_1), \ldots ,f(X_n)$ are independent.

    Take $n=2$. Let $\mu_i = P \circ X_{i}^{-1}$
    \begin{align*}
        E[f_1(X_{i})f_2(X_{2})] &= \int_{\mathbb{R}^{2}}f_1(x_1)f_2(x_2)(\mu_1 \otimes \mu_2)(dx_1 dx_2)\\
        &= \int_{R}\mu_2(dx_2)\int_{\mathbb{R}}\mu_1(dx_1)f_1(x_1)f_2(x_2)\\
        &= \int _{\mathbb{R}}u_2(dx_2)f_2(x_2)\int_{\mathbb{R}}u_1(x_1)f_1(x_1)\\
        &= E\Big[f_1(X_1) \Big]E\Big[f_2(X_2)\Big]
    \end{align*}
\end{proof}
\begin{remark}
    It's OK to mix notation: if $X \independent Y$, then 
    \begin{align*}
        E\Big[g(X,Y)\Big] = \int g \, d \mu\otimes \nu = \int \nu(dy)\int \mu(dx) g(x,y)\\
        = \int \nu(dy)\mathbb{E}\Big[g(X,y)\Big]
    \end{align*}
\end{remark}

\begin{corollary}
    Let $X = (X_1, \ldots ,X_n)$ have PDF $f$ on $\mathbb{R}^{n}$, and let $f_i$ be PDF of
    $X_{i}$ for $i=1, \ldots ,n$. Then
    \begin{align*}
        X_1, \ldots ,X_{n} \text{ are independent} \iff f(x_1, \ldots ,x_n) = \prod_{i=1}^{n}f_i(x_i)
        \quad \text{for Lebesgue almost every} (x_1, \ldots ,x_n) \in \mathbb{R} 
    \end{align*}
\end{corollary}

\begin{definition}[convolutions]
    Let $\mu, v$ be Borel probability measure on $\mathbb{R}$. Their $\underline{\text{
        convolution
    }}$ is 
    \begin{align*}
        \mu * \nu(B) = \int_{\mathbb{R}} \mu(B-x)\nu(dx), \quad B \in \mathcal{B}_{\mathbb{R}}
    \end{align*}
\end{definition}

Why is $\mu(B-x)$ is measurable?

$\mu(B-x) = \int_{\mathbb{R}}\1_{B-x}(y)\mu(dy) = \int_{\mathbb{R}}
\underbrace{\1_{B}(x+y)}_\text{jointly measurable function}(x,y)\mu(dy)$

Fubini $\Longrightarrow$ the intepretation over $y$ leaves a measurable function of the variable $x$.

\vspace{10mm}
We consider the probability meaning of $\mu*v$:
\begin{align*}
    \mu * \nu(B) &= \int_{\mathbb{R}}\Big[\int_{\mathbb{R}}\1_{B}(x+y)\mu(dx) \Big]\nu(dy)\\
    &= \int_{\mathbb{R}^{2}}\1_{B}(x+y)(\mu \otimes \nu)(dx,dy)\\
    &= \mathbb{E}\Big[\1_{B}(X+Y)\Big]\\
    &= P(X+Y \in B)
\end{align*}
Let $X \independent Y$, $X \sim  \mu, Y \sim  \nu$. Then we have $(X,Y) \sim  \mu\otimes \nu$

\begin{theorem}
    $X \independent Y$, $X \sim  \mu$, $Y \sim \mu $ $\Longrightarrow X+Y \sim  \mu\times \nu$
\end{theorem}

What happened 

Suppose $\mu$ has PDF $f$, $\nu$ has PDF $g$. Find 
\begin{align*}
    \mu * \nu(A) = \int_{\mathbb{R}^{2}}\1_{A}(x+y)\mu(dx)\nu(dy)\\
    = \int_{\mathbb{R}^{2}}\1_{A}(x+y)f(x)g(y), dxdy\\
    = \int_{\mathbb{R}}dxf(x) \int_{\mathbb{R}}\1_{A}(x+y)g(y)\, dy\\
    = \int_{\mathbb{R}}dxf(x) \int_{\mathbb{R}}\1_{A}(y)(y-x)\,dy\\
    = \int_{\mathbb{R}}dy\1_{A}(y) \int_{\mathbb{R}}dx f(x)g(y-x)
\end{align*}
By definition $f*g(y)$ we see that is the PDF of $\mu * v$

\begin{example}
    Gaussian density: $f(x_1) = \frac{1}{\sqrt{2\pi\sigma_1^{2}}}e^{-\frac{(x-m_1)}{
        2\sigma_1^{2}
    }}$ and $f(x_2) = \frac{1}{\sqrt{2\pi\sigma_2^{2}}} e^{-\frac{(x-m_2)^{2}}{2\sigma_2^{2}}}$.

    We have 
    \begin{align*}
        (f_1 * f_2)(x) = \frac{1}{\sqrt{2\pi (\sigma_1^{2} + \sigma_2^{2})}}
        e^{-\frac{(x - m_1 - m_2)^{2}}{2(\sigma_1^{2}+ \sigma_2^{2})}}
    \end{align*}
\end{example}

\DATE{Sep 19, 2024}

\subsubsection{Construction of probability spaces with desired independent r.v.'s}
2.1.4 section in Durett

Finite case: Given $\mu_1, \ldots ,\mu_n$ Borel probability measure on $\mathbb{R}$.

$\underline{\text{Want:}}$ independent r.v,'s $X_1, \ldots ,X_{n}$ with $X_{i} \sim \mu_1$

Take $\Omega = \mathbb{R}^{n} = \{ \omega = (x_1, \ldots ,x_n): x_i \in \mathbb{R} \}$.
$\mathcal{F} = \mathcal{B}_{\mathbb{R}} = \mathcal{B}_{\mathbb{R}}^{\otimes n}$ (
    $X_{i}$ has probability distribution $\mu_i$
),
$P = \otimes_{i=1}^{n}\mu_i$, $X_{i}(\omega) = x_i$ ("coordinate r.v.'s coordinate projections").

Given $B_1, \ldots ,B_n \in \mathcal{B}_{\mathbb{R}}$
\begin{align*}
    P(X_{1} \in B_1, \ldots ,X_n \in B_n) &= P\{ \omega \in \Omega: X_{i}(\omega) \in B_1, \ldots ,
    X_n(\omega) \in B_n \}\\
    &= (\otimes_{i=1}^{n}\mu_i)\{ (x_1, \ldots ,x_n) \in \mathbb{R}^{n}: x_1 \in B_1, \ldots ,
    x_n \in B_n\}\\
    &= (\otimes_{i=1}^{n}\mu_i)(\prod_{i=1}^{n}B_i )\\
    &= \prod_{i=1}^{n}\mu_i(B_i)\\
    &= \prod_{i=1}^{n} P(X_{i} \in B_i) \quad \text{ by }~\ref{eq:1}
\end{align*}
Intermediate step: pick $j$, take $B_i = \mathbb{R}$ for $i \neq j$, substitute with the calculation:
\begin{equation} \label{eq:1}
    \begin{aligned}
        P(X_{j} \in B_j) = \prod_{i=1}^{n}  \mu_i(B_i) = \mu_{j}(B_j) \\
        \Longrightarrow X_j \sim \mu_j
    \end{aligned}
\end{equation}
This is all works if we replace and $\mathbb{R}, \mathcal{B}_{\mathbb{R}}$ with arbitrary measurable
spaces $(S_i, \mathcal{A}_{i})$. The choice of $(\Omega, \mathcal{F}, P)$ is not unique at all!

\begin{definition}[Infinite case]
    A $\underline{\text{stochastic process}}$ is an dexed collection 
    $\{ X_{\alpha}: \alpha \in \mathcal{J} \}$ of r.v.'s all defined on the $\underline{\text{same}}$
    $(\Omega, \mathcal{F}, P)$.
\end{definition}

\begin{theorem}[Kolmogorov's Extension Theorem]
    (for index set $\mathbb{Z}_{\ge 0}$) Assume that $\forall n \ge 1$, we have a probability measure
    $\vu_{n}$ on $(\mathbb{R}^{n}, \mathcal{B}_{\mathbb{R}^{n}})$ and these measures are 
    consistent: $\forall  B \in \mathcal{B}_{\mathbb{R}^{n}}: \vu_{n+1} = 
    \vu_{n}(B)$

    Let $\Omega = \mathbb{R}^{\mathbb{Z} \ge 0} = \{ \omega = (x_i)_{i=1}^{\infty}: 
    \text{ each } x_i \in \mathbb{R} \}$, $\mathcal{F}$ = product $\sigma$-algebra = 
    $\sigma \{ A_1 \times \cdots \times A_n \times \mathbb{R}
    \times \mathbb{R} \cdots: n \in \mathbb{Z}_{> 0}, A_1, 
    \ldots ,A_n \in \mathcal{B}_{\mathbb{R}} \} =$ $\sigma$-algebra generated by the projection mapping
    $X_{i}(\omega) =x_i, \; i \in \mathbb{Z}_{>0},\,=$ smallest $\sigma$-algebra on $\Omega$ under
    which each $X_{i}: \Omega \to \mathbb{R}$ is measurable.

    Then $\exists $ unique probability measure $P$ on $\Omega$ such that $P \{ 
        \omega \in \Omega: (X_{1}(\omega), \ldots ,X_n(\omega) \in B) = \vu_{n}(B) \quad
        \forall n \in \mathbb{Z}_{>0}, B \in \mathcal{B}_{\mathbb{R}^{n}}
     \}$
\end{theorem}

\begin{theorem}[Kolmogorov Extension theorem process version]
    Given consistent finite-dim distribution $\{ \vu_{n} \}_{n \ge 1}$ on $\mathbb{R}^{n} \quad
    \forall n$, $\exists $ a stochastic process $(X_{k})_{k \in \mathbb{Z}_{>0}}$ with marginal
    $(X_1, \ldots ,X_n) \sim \vu_{n}$
\end{theorem}
\begin{proof}
    Take the coordinate process from the previous theorem.
\end{proof}

$\underline{\text{Generalizations:}}$ 
\begin{enumerate}
    \item Instead of $\mathbb{R}$, we can take any Borel subsets of 
    $\underbrace{\text{complete separable metric spaces.}}_{\text{"Polish spaces"}}$.
    \item The index set can be totally arbitrary. [cf. Dudley's book]
\end{enumerate}

To produce a process $(X_k)_{k \in \mathbb{Z}_{>0}}$ of independent r.v.'s with $X_k \sim  \mu_k$,
take $\vu_{n} = \mu_1 \otimes\cdots\otimes \mu_n$ in K's extension theorem.

\begin{definition}
    An IID process is a process of independent $\underline{\text{identically distributed }}$ r.v.'s.
\end{definition}


\subsection{Strong Law Large Number (2.4 in Durett)}

Two big goals for IID process $\{ X_k \}_{k \in \mathbb{Z} > 0}$
\begin{theorem}
    If $\mathbb{E}|X_1| < \infty$, then $S_n = X_1+ \cdots +X_n$ satisfies
    \begin{align*}
        \frac{S_n}{n} \longrightarrow \mathbb{E}X_1 \quad \text{w.p.}1
    \end{align*}
\end{theorem}

\begin{theorem}
    Central Limit Theorem: if $\sigma^{2} = \text{Var}(X_1) < \infty $, then 
    \begin{align*}
        P \{  \frac{S_n - n\mathbb{E}X_1}{\sigma \sqrt{n}} \le s \} \xrightarrow{n \to \infty}
        \int_{-\infty}^{s}\frac{e^{\frac{-x^{2}}{2}}}{\sqrt{2 \pi}}\,dx
    \end{align*}
\end{theorem}

\begin{definition}
    Let $\{ A_n \}$ be a sequence of events in $(\Omega, \mathcal{F}, P)$.
    \begin{align*}
        \{ A_n \text{i.o.(inifinitly often)} \} = \{ \omega \in \Omega: \omega \in A_n
        \text{ for infinityly many }n \}\\
        = \{ \omega \in \Omega: \forall m \ge 1, \exists n \ge m \text{ s.t. } \omega \in A_n \}\\
        = \bigcap _{m\ge 1}\bigcup _{n\ge m} A_n(= \bar{lim}A_n)
    \end{align*}
\end{definition}

\begin{theorem}[1st Borel-Cantelli Lemmas]
    \begin{align*}
        \sum_{n=1}^{\infty}P(A_n) < \infty \Longrightarrow P(A_n \text{ i.o.}) = 0
    \end{align*}
\end{theorem}
\begin{proof}[(1.)]
    Let $N(\omega) = \sum_{n=1}^{\infty}\1_{A_n}(\omega) = \#. $ of events that occur.
    \begin{align*}
        \mathbb{E}[N] \overset{\text{MCT}}{=} \sum_{n=1}^{\infty}E[\1_{A_n}] = \sum_{n=1}^{\infty}P(A_n) < \infty\\
        \Longrightarrow P(N=\infty) = 0
    \end{align*}
\end{proof}
\begin{proof}[(2.)]
    \begin{align*}
        P(\bigcap _{m\ge 1}\bigcup _{n\ge m} A_n) = \lim_{m \to \infty}P(\bigcup_{n\ge m}A_n)\\
        \le \lim_{n \to \infty}\sum_{n\ge m}P(A_n) = 0 \quad \text{ by convergent series tails}
    \end{align*}
\end{proof}
\begin{definition}
    (Suppose all defined on the same probability space) $X_n \xrightarrow{a.s.} X$ if $P\{ 
        \omega \in \Omega: \lim_{n \to \infty}X_n(\omega) = X(\omega)
     \}$ = 1
\end{definition}
\begin{lemma}
    Suppose $\forall  \epsilon > 0, \, \sum_{n=1}^{\infty}P(|x_n-x| \ge \epsilon) < \infty$.

    Then $X_n \longrightarrow X \quad a.s.$.
\end{lemma}
\begin{proof}
    Pick any sequences $0 < \epsilon_{j} \searrow$
    \begin{align*}
        \text{B-C} \Longrightarrow P(\bigcap _{m\ge 1}\bigcup _{n\ge m}\{ |X_n-X| \ge \epsilon_j \}) &= 0\\
        \Longrightarrow &1=  P(\bigcup_{m\ge 1}\bigcap_{n\ge m}\{ |X_n - X| < \epsilon_j \})\\
        &= P\{ \exists m < \infty \; s.t. \; n\ge m \Longrightarrow
        |X_n-X| < \epsilon_j \}\\
        1 &= P(\bigcap _{j=1}^{\infty}\bigcup _{m\ge 1}\bigcap_{n\ge m}\{ |X_n-X| < \epsilon_j \})\\
        &= P\{ \forall j \;\exists m,\, m,n \ge m \Longrightarrow|X_n-X| < \epsilon_j \}\\
        &= P\{ X_n \to X \}
    \end{align*}
\end{proof}
\begin{example}
    Suppose $\mathbb{E}|Y_n| \le 2^{-n}$. Then $Y_n \xrightarrow{a.s.}0$.
    \begin{proof}
        $\sum_{n\ge 1}P(|Y_n|\ge \epsilon) \le \sum_{n=1}^{\infty}\frac{E|Y_n|}{\epsilon}
        \le \frac{1}{\epsilon} \sum_{n=1}^{\infty}2^{-n} < \infty$
    \end{proof}
\end{example}
\begin{remark}[Markov-Chebyshev]
    Suppose r.v. $Z > \ge 0$, $a>0$:
    \begin{align*}
        P(Z \ge a) = \mathbb{E}[\1_{Z\ge a}]\le \mathbb{E}[\frac{Z}{a}\1_{Z\ge a}] \overset{Z\ge_0}{\le}
        \mathbb{E}[\frac{Z}{a}] 
    \end{align*}
\end{remark}

\end{document}