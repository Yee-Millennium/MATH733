\documentclass[11pt]{article}

\usepackage{preamble}
\input{math_commands.tex}

\title{Notes of Math 733: Probability Theory\\ Instructor: Timo Seppäläinen}
\author{YI WEI}
\date{Sep 2024}


\begin{document}

\maketitle
\tableofcontents



\section{Probability Space}
\DATE{Sep 5,2024}

Setup (Undergraduate level):
\begin{align*}
    &\Omega \text{ sample space: set of all the individual outcomes} \\
    &\mathcal{F}\text{ event space: appropriate collection of subsets of } \Omega\\
    &P: \text{a function on a subsets of } \Omega, P(A)= \text{ the probability of the set (event) }A 
\end{align*}

\begin{axiom}
    \begin{align*}
        P(\bigcup_k A_k) = \sum_{k} P(A_k) \quad \text{whenever } {A_k} \text{ is a pairwise disjoint sequence of events} 
    \end{align*}
\end{axiom}

\begin{example}
    \, 

    \begin{enumerate}
        \item roll a dice: $ \Omega = \{1,2,3,4,5,6 \}$, 
        $\mathcal{F} = \mathcal{P}(\Omega) = \text{ power set of }
        \; \Omega = \text{ collection of all subset of} \; \Omega$
        \item \# of customers to a service station in some fixed time interval 
                \begin{align*}
                    \Omega &= \mathbb{Z}_{\ge 0} \\
                    \mathcal{F} &= \mathcal{P}(\Omega)\\
                    P(k) &= e^{-\lambda }\frac{\lambda^k}{k!} \quad \text{for }k \in \Omega\\
                    P(A) &= \sum_{k \in A} e^{-\lambda }\frac{\lambda^k}{k!} \quad \text{for } A \subseteq \Omega
                \end{align*}
        \item Choose uniformly random real number from $[0,1]$ \\
                $P(x) = 0 \quad \forall \, x \in [0,1]$ \\
                if $0 \le a < b \le 1$:
                \begin{align*}
                    P([a,b]) = b-a
                \end{align*}
        \item \label{example:1.1.4} 
                Flip a fair coin for infinitly many times, $0 = \text{heads}, \, 1=\text{tails}$:
                \begin{align*}
                    &\Omega = \{0,1\}^{\mathbb{Z}_{\ge 0}} \\
                    &P\{w: x_1 = a_1, x_2=a_2, \ldots , x_n = a_n\} = 2^{-n} \label{*} \tag{*}
                \end{align*}
                From this: $P\{w\} = 0 \quad \forall w \in \Omega$
                \begin{exercise}
                    how to prove $\Omega$ is uncountable: diagonal principle
                \end{exercise}
    \end{enumerate}
\end{example}

\begin{definition}
    Let $X$ be a space. A $\sigma$-algebra on $X$ is a collection $\mathcal{A}$ of subsets of $X$ that
    satisfies these properties:
    \begin{enumerate}
        \item $\emptyset \in \mathcal{A}$
        \item $A \in \mathcal{A} \Longrightarrow  A^C \in \mathcal{A}$
        \item $\{A_k \}_{k=1}^{\infty} \Longrightarrow \bigcup_{k=1}^{\infty}A_k \in \mathcal{A}$ 
    \end{enumerate}
    And we call $(X, \mathcal{A})$ is a $\underline{\text{measurable space}}$.
\end{definition}

\begin{definition}
    Given $(X, \mathcal{A})$
    A measure is a function $u: \mathcal{A} \to [0, \infty]$ such that:
    \begin{enumerate}
        \item $P(\emptyset) = 0$
        \item $u( \bigcup_{k}A_k) = \sum_{k=1}u(A_k)$ for a pairwise disjoint sequence $\{A_k\}_k \subseteq \mathcal{A}$
    \end{enumerate}
    $(X, \mathcal{A}, u)$ is a $\underline{\text{measure space}}$.
\end{definition}

\begin{definition}
    If $X$ is a metric space, its $\underline{\text{Borel } \sigma\text{-algebra}} \,\, \mathcal{B}_{X}$ is by
    definition the smallest $\sigma$-algebra containing all the OPEN subsets of $X$.
\end{definition}

\begin{definition}
    $\underline{\text{Lebesgue measure}} \; m$ on $\mathbb{R}^d$ is the measure that satisfies 
    \begin{align*}
        m\Big( \prod_{i=1}^{d} [a_i,b_i]\Big) = \prod_{i=1}^{d} (b_i-a_i)
    \end{align*}
\end{definition}

\begin{definition}
    A $\underline{\text{probability space}} \; (\Omega, \mathcal{F}, P)$ is a measure space such that $P(\Omega) = 1$.
\end{definition}

\begin{example}
    Example of product $\sigma$-algebra from example 1.1.~\ref{example:1.1.4}: \\
    $\mathcal{F}$ = product $\sigma$-algebra $=$ samllest $\sigma$ -algebra that contains all sets of the type
    \begin{align*}
        \{w: x_1 = a_1, \ldots , x_{n} = a_{n}\} \quad , n \in \mathbb{Z}_{>0}, a_{1}, \ldots ,a_n \in \{0,1 \}.
    \end{align*}
    $P$ obtained from Eq.~\ref{*}
\end{example}

\begin{definition}
    Let $(X, \mathcal{A}), \, (Y, \mathcal{B})$ be measurable space, and $f: X \to Y$ be a function.\\
    We say $f$ is a $\underline{\text{measurable function}}$ if:
    \begin{align*}
        f^{-1}(B) = \{x \in X: f(x) \in \mathcal{B} \} \subseteq \mathcal{A}, \quad \forall B \in \mathcal{B}
    \end{align*}
    A $\underline{\text{random variable}} \; X$ is a measurable function:
    \begin{align*}
        X: (\Omega, \mathcal{F}) \to (\mathbb{R}, \mathcal{B}_{\mathbb{R}})
    \end{align*}
\end{definition}

\begin{example}
    flip of a fair coin $\Omega = \{ w = (x_1,x_2): x_1,x_2 \in \{0,1\}\}$, $0 = \text{heads}, \, 1=\text{tails}$:
    \begin{align*}
        X_1(w) &= x_1  \quad \text{outcome of the first flip}\\
        X_2(w) &= x_2  \quad \text{outcome of the second flip}
    \end{align*}
    We define $Y(w) = X_1(w) + X_2(w)$ = \# of tails in the two flips \\
    The information contained in $Y(w)$ is represented by $\sigma$-algebra generated by $Y$ defined as follows:
    \begin{align*}
        \sigma(Y) &= \{\{Y \in B\}: B \in \mathcal{B}_{\mathbb{R}}\}\\
        &= \Big\{\emptyset, \Omega, \{(0,0)\}, \{(0,1),(1,0) \} , \{(1,1)\} \text{ and the unions of these sets}\Big\} \subsetneq \mathcal{F}
    \end{align*}
\end{example}


\DATE{Sep 10,2024}
\begin{enumerate}
    \item push-forward: $(X, \mathcal{A}, \mu)$ is a measure space, 
    and $(Y, \mathcal{B})$ is a measurable space. And there is a $f: X \to Y$.
    The push-forward of $\mu$ is the measure $v$ on $(Y, \mathcal{B})$ defined by $v(\mathcal{B}) =
    u(f^{-1}(\mathcal{B}))$
    \begin{exercise}
        Check $v$ is a measure.
    \end{exercise}

    \item Absolute continuity: Let $\mu, \lambda$ be measures on $(X, \mathcal{A})$. Then $\mu$ is absolute
    continuous w.r.t $\lambda$ if $\lambda(A) = 0 \Longrightarrow \mu(A) = 0 \quad \forall  A \in \mathcal{A}$.
    \remark $\mu \ll \lambda$. If $\mu \ll \lambda$, then there exists a measurable function 
    $f: X \to \mathbb{R}_{\ge 0}$ s.t. 
    \begin{align*}
        \mu(A) = \int_{A} f \, d \lambda \qquad \forall A \in \mathcal{A}
    \end{align*}
    This is called Radom-Nikodym derivative $f(x) = \frac{d \mu}{d \lambda}(x)$
\end{enumerate}

\begin{definition}
    Let $X: (\Omega, \mathcal{F}, P) \to (\mathbb{R}, \mathcal{B}_{\mathbb{R}})$ be a random variable.
    The $\underline{\text{distribution}}$ of $X$ is the $\mu = P \circ X^{-1}$, i.e.,
    \begin{align*}
        \mu (B) = P\{w \in \Omega: X(w) \in B\} \qquad \text{for } B \in \mathcal{B}
    \end{align*}
    In short: $P\{X \in B\} = P(X \in B)$
\end{definition}

\begin{definition}
    The CDF of $X$ is the function $F$ on $\mathbb{R}$ defined by 
    \begin{align*}
        F(x) = P(X \le x) = \mu(-\infty,x]
    \end{align*}
\end{definition}
\begin{definition}
    If $\mu \ll $ Lebegue measure, then $X$ has a density function $f$ which satisfies 
    \begin{align*}
        P(a < X \le b) = \int _{a}^{b} f(x) \, dx = \mu(a,b] = F(b) - F(a)
    \end{align*}
\end{definition}

\begin{remark}
    A $\underline{\text{discrete random variable}}$ has at most countably many values, and since individual pts have
    positive probability
    \begin{align*}
        \mu \{k\} = P(X=k) > 0 = leb\{x\}
    \end{align*}
    Then we know $\mu \ll $Leb fails and X has no density function.
\end{remark}


\begin{definition}
    The $\underline{\text{expectation}}$ of a r.v. $X$ is defined by 
    \begin{align*}
        EX = \int_{\Omega} X \, dP
    \end{align*}
    \begin{remark}
        Abstract Lebesgue integral on $(\Omega, \mathcal{F}, P)$
    \end{remark}
\end{definition}

% \begin{definition}
%     If $A \in \mathcal{F}$ is an event, its indicator r.v. is 
%     \begin{align*}
%         \1 _{A}(w) = 
%         \begin{cases}
%             \item 0, w \not A 
%             \item 1, w \in A
%         \end{cases}
%     \end{align*}
% \end{definition}
\begin{definition}
    If $A \in \mathcal{F}$ is an event, its indicator random variable is 
    \[
    \1 _{A}(w) = 
    \begin{cases} 
        1, & \text{if } w \in A, \\
        0, & \text{if } w \notin A.
    \end{cases}
    \]
\end{definition}

We know 
\begin{align*}
    E[\1_{A}] &= 0 \cdot P\{\1_{A} = 0\} + 1 \cdot  P\{\1_{A} = 1\}\\
    &= P(A)
\end{align*}

\begin{example}
    \item $X \sim Poisson(\lambda) \Longrightarrow E[g(X)] = \sum_{k=0}^{\infty} g(k) 
    \frac{e^{-\lambda}\lambda^k}{k!}$
    \item $X \sim  Exp(\lambda) \Longrightarrow E[g(X)] = \int_{0}^{\infty} g(x) \lambda 
    e^{-\lambda } \mathrm{d}x$
\end{example}


\begin{theorem}
    \,

    $\underline{\text{Key result}}$: 
    $$E[f(X)] := \int_{\Omega}f(X)  \mathrm{d}P = \int_{\mathbb{R}}f \, d \mu$$

    Here: $X$ is a r.v. on $(\Omega, \mathcal{F}, P)$, $\mu = P \circ X^{-1}=$ distribution of $X$ ,
    $f: \mathbb{R} \to \mathbb{R}$ is a Borel function $f(X(w)) = (f \circ X)(w)$
\end{theorem}
\begin{proof}
    \,

    \begin{enumerate}
        \item $f = \1_{B}, B \in \mathcal{B}_{\mathbb{R}}$.
        \begin{remark}
            Notation: $\int_{\Omega} \1_{B}(X(w))  P(\mathrm{d}w)$ (same as $dP(w)$)
            \begin{align*}
                \int_{\Omega} \1_{B}(X(w))  P(\mathrm{d}w) &= \int_{\Omega}\1_{X^{-1}(\mathcal{B})}(w)\mathrm{d}x\\
                &= P(X^{-1}(B)) = \mu(B) = \int_{\mathbb{R}} \1_{B} d \mu
            \end{align*}
        \end{remark}
        \item $f = \sum_{i=1}^{n} a_{i} \1_{B_{i}}$, $a_1, \ldots ,a_n \in \mathbb{R}$, $B_1, \ldots ,B_n \in \mathcal{B}_{\mathbb{R}}$
        \begin{align*}
            \int_{\Omega} \sum_{i=1}^{n}a_{i}\1_{B_i}(X) \, dP 
            &= \sum_{i=1}^{n}a_{i}  \int_{\Omega} \1_{B_i}(X) \, dP\\
            &= \sum_{i=1}^{n}a_i \int_{\mathbb{R}} \1_{B_i}\, d \mu \\
            &= \int_{R} \sum_{i=1}^{n}a_i \1_{B_i} \, d \mu
        \end{align*}
        \item $f \ge 0, \exists $ simple function $0 \le f_n $
        \begin{align*}
            \int _{\Omega} f(X) \, dP &= \lim_{n \to \infty} \int_{\Omega} f_n(X) \, dP \qquad (M.C.T.)\\
            &= \lim_{n \to \infty} \int_{\mathbb{R}} f_n \, d \mu \\
            &= \int_{\mathbb{R}} f \, d \mu
        \end{align*}
        \begin{remark}
            $f_n(x) = \sum_{k=0}^{n(2^n - 1)} \frac{k}{2^n} \1 \{\frac{k}{2^n} \le f(x) < \frac{k+1}{2^n}\}
            + n \1 \{ f(x) > n\}$
        \end{remark}
        \item For general $f: \mathbb{R} \to \mathbb{R} = f^{+} - f^{-}$ Borel function where $f^{+}, f^{-} \ge 0$
        \begin{align*}
            \int _{\Omega} f(X) \, dP &= \int _{\Omega} f^{+}(X) \, dP - \int _{\Omega} f^{-}(X) \, dP\\
            &= \int _{\mathbb{R}} f^{+} \, d \mu - \int _{\mathbb{R}} f^{-} \, d \mu \\
            &= \int_{\mathbb{R}} f \, d \mu
        \end{align*}
    \end{enumerate}
\end{proof}

\begin{example}
    \begin{enumerate}
        \item $X \sim Possion(\lambda)$, $\mu = $ distribution of $X$.
        We know $\mu(B) = \sum_{k: k\in B} e^{-\lambda} \frac{\lambda^k}{k!} \Longrightarrow
        \mu(\mathbb{R} \setminus{\mathbb{Z}_{\ge 0}})= 0$. Then we have:
        \begin{align*}
            E[e^{-tX}] &= \int_{\mathbb{R}} e^{-tx} \mu(dx)\\
            &=\int_{\mathbb{Z}_{\ge 0}} e^{-tx} \mu(dx) \\
            &= \sum_{k \in \mathbb{Z}_{k\ge 0}} \int_{\{k\}} e^{-tx} \, \mu(dx)\\
            &= \sum_{k \ge 0} e^{-tk} e^{-\lambda} \frac{\lambda^k}{k!}\\
            &= e^{-\lambda} e^{\lambda e^{-t}} \\
            &= e^{\lambda(e^{-t}-1)}
        \end{align*}
        \item $X \sim Exp(\lambda)$
        \begin{align*}
            E[e^{-tX}] &= \int_{\mathbb{R}} e^{-tx} \, \mu(dx) = \int_{[0, \infty)} e^{-tx} \lambda e^{-\lambda x} \, dx \\
            &= \lim_{M \to \infty} \int_{[0,M]} \lambda e^{-(t+\lambda)x} \, dx = \lim_{M \to \infty}
            R \int_{0}^{M} \lambda e^{-(t+\lambda)x} \mathrm{d}x\\
            &= \lim_{M \to \infty} (-\frac{\lambda}{t+\lambda}) e^{-(t+\lambda)x} |^M_{0} \\
            &= \lim_{M \to \infty} \Big( (-\frac{\lambda}{t+\lambda}) e^{-(t+\lambda)M} + \frac{\lambda}{t+\lambda} \Big)\\
            &= \frac{\lambda}{t+\lambda}
        \end{align*}
    \end{enumerate}
\end{example}


\newpage
\section{Laws of Large Numbers}

\DATE{Sep 12, 2024}
\subsection{Independence}

Perhaps you recall this: events $A$ and $B$ are independent if $P(A \cap B) = P(A)P(B)$

\begin{definition}
    Let $\Omega, \mathcal{F}, P$ be a probability space. 

    Let $\mathcal{F}_1, \ldots ,\mathcal{F}_n$ be sub-$\sigma$-algebra of $\mathcal{F}$. (Means
    each $\mathcal{F}_i$ is a $\sigma$-algebra and $\mathcal{F}_i \subseteq \mathcal{F}$)

    Then we say $\mathcal{F}_1, \ldots ,\mathcal{F}_n$ are independent if $\forall A_1 \in \mathcal{F}_1,
    A_2 \in \mathcal{F}_2, \ldots ,A_n \in \mathcal{F}_n$, then
    \begin{align*}
        P(\bigcap_{i=1}^{n} A_i) = \prod_{i=1}^{n}P(A_i) 
    \end{align*}
    Now r.v.'s $X_1, \ldots ,X_n$ on $\Omega$ are independent if the $\sigma$-algebra 
    $\sigma(X_1), \ldots ,\sigma(X_n)$ are independent. Equivalently, $\forall $ measurable 
    sets in the range space,
    \begin{align*}
       P(\bigcap_{i=1}^{n}\{X_{i} \in B_i\}) = \prod_{i=1}^{n} P\{X_{i} \in B_i\} 
    \end{align*}
\end{definition}

Events $A_1, \ldots ,A_n$ are independent if the r.v.'s $\1_{A_1}, \ldots ,\1_{A_n}$ are independent.

And arbitrary collection $\{\mathcal{F}_{\beta}: \beta \in \mathcal{J}\}$ of sub-$\sigma$-algebra is independent if
$\forall \text{distinct } \beta_1, \ldots ,\beta_{n} \in \mathcal{J}$, $\mathcal{F}_{\beta_{1}}, \ldots ,\mathcal{F}_{\beta_{n}}$
are independent.

\begin{claim}
    $\underline{\text{Fact:}}$ $X_1, \ldots ,X_n$ are independent, then so are 
    $f_1(X_1), \ldots ,f_n(X_n)$
\end{claim}

\begin{remark}
    Why product?

    $X,Y$ discrete r.v.'s. We're interested in the event $\{X=k\}$. Suppose we learn that $Y=m$.
    We replace P with $P(\cdot , Y=m)$ defined by $P(A| Y=m) = \frac{P(A \bigcap \{Y=m\})}{P(Y=m)}$

    When is $P(X=k) = P(X=k | Y=m)$?

    \begin{align*}
        P(X=k) &= P(X=k | Y=m)\\
        \iff P(X=k)P(Y=m) &= P(X=k,Y=m)
    \end{align*}
\end{remark}

We need some notions/tool to check easily if two r.v.'s are independent.
\begin{enumerate}
    \item Develop a simpler criterion for checking independence of a given collection of r.v.'s.
    \item To construct a probability space with desired independent r.v.'s.
\end{enumerate}

\begin{example}
    Let $X_1, X_2,X_3$ be independent $Bernolli(p)$ r.v.'s.
    \begin{align*}
        P(X_{i} = 1) = p = 1-P(X_{i} = 0)
    \end{align*}
    Consider the following events:
    \begin{equation*}
        \begin{cases} 
        \{X_1 + X_2 = 1\}\\ 
        \{X_2+X_3 = 1 \}  
        \end{cases}
    \end{equation*}
    Firstly we have:
    \begin{align*}
        P(X_1 + X_2 = 1) = P(01) + P(10) = 2p(1-p) = P(X_2+X_3 = 1)
    \end{align*}
    And we have:
    \begin{align*}
        P(X_1+X_2=1, X_2+X_3=1) = P(101) + P(010) = p^{2}(1-p) + p(1-p)^2 = p(1-p)
    \end{align*}
    If the two events and independent, we have:
    \begin{align*}
        P(X_1+X_2=1, X_2+X_3=1) &= P(X_1 + X_2 = 1) \cdot P(X_2+X_3 = 1)\\
        \iff p(1-p) &= 4p^{2}(1-p)^{2} \\
        \iff p(1-p) &= \frac{1}{4}, \; p =0,\; \text{or} \; p=1 \\
        \iff p = \frac{1}{2},\; &0,\; \text{or} \; 1
    \end{align*}
\end{example}


\begin{theorem}
    \label{theorem:2.2}
    Let $\mathcal{A}_{1}, \ldots ,\mathcal{A}_{n}$ be subcollection of $\mathcal{F}$,
    Assume that each $\mathcal{A}_{i}$ is closed under intersection, which means 
    $(A,B \in \mathcal{A}_{i} \Longrightarrow A \bigcap B \in \mathcal{A}_{i})$ and $\Omega \in \mathcal{A}_{i}$.
    Assume that the probability $P(\bigcap_{i=1}^{n} A_{i}) = \prod_{i=1}^{n}P(A_i) \quad
    \forall A_1 \in \mathcal{A}_{1}, \ldots ,A_n \in \mathcal{A}_{n}$. Then the $\sigma$-algebra 
    $\sigma(\mathcal{A}_{1}), \ldots ,\sigma(\mathcal{A}_{n})$ are independent.
\end{theorem}
\begin{example}
    Collection of sets which can generate Borel-algebra:

    $A_i = \{(a,b): -\infty < a < b <\infty\}$, then $\sigma(A_{i}) = \mathcal{B}_{\mathbb{R}}$.

    Or you can take $(-\infty, b]$ ......
\end{example}

\vspace*{5mm}
The tool for proving the theorem: $\underline{\text{Dynkin's} \;\pi-\lambda \text{ theorem}}$.

\begin{definition}
    Let $\mathcal{A}$ be a collection of subset of $\Omega$
    \begin{enumerate}
        \item $\mathcal{A}$ is a $\pi$-system if it is closed under intersections.
        \item $\mathcal{A}$ is a $\lambda$-system if it has the following three properties:
        \begin{enumerate}
            \item $\Omega \in \mathcal{A}$
            \item $\forall A,B \in \mathcal{A}$ and $A \subseteq B \Longrightarrow B \setminus A \in \mathcal{A}$
            \item If $A_1 \subseteq A_2 \cdots \subseteq A_n \subseteq 
            \cdots$ and each $A_{i} \in \mathcal{A}$, then
            $\bigcup_{i=1}^{\infty} A_{i} \in \mathcal{A}$
        \end{enumerate}
    \end{enumerate}
\end{definition}

\begin{theorem}
    \label{theorem:2.3}
    Suppose $\mathcal{P}$ is a $\pi$-system, $\mathcal{L}$ is a $\lambda$-system and $\mathcal{P} \subseteq \mathcal{L}$,
    then $\sigma(\mathcal{P}) \subseteq \mathcal{L}$
\end{theorem}
We use theorem~\ref{theorem:2.3} to prove theorem~\ref{theorem:2.2}.

\vspace*{7mm}
\begin{proof}[Proof of theorem 2.2:]
    \,
    
    Fix $A_2 \in \mathcal{A}_{2}, \ldots ,A_n \in \mathcal{A}_{n}$, set 
    $\mathcal{F} = A_2 \bigcap \cdots \bigcap A_n$
    \begin{align*}
        \mathcal{L} = \{ A \in \mathcal{F}: P(A \bigcap F) = P(A)P(F) \}
    \end{align*}
    \begin{claim}
        $\mathcal{A}_1 \subseteq \mathcal{L}$.
        \begin{proof}[Proof of Claim 2.4]
            \,

            Check that $P(F) = \prod_{i=2}^{n}P(A_{i}) $

            Take $A_1 = \Omega$

            Let $A_1 \in \mathcal{A}_{1}$. $P(A_{1} \bigcap F) = P(\bigcap_{i=1}^{n}A_{i})
            = \prod_{i=1}^{n}P(A_{i}) = P(A_{i})P(F) $
        \end{proof}
    \end{claim}
    \begin{claim}
        $\mathcal{L}$ is a $\lambda$-system.
        \begin{proof}[Proof of Claim 2.5]
            \,

            \begin{enumerate}
                \item $\Omega \in \mathcal{A}_{1} \subseteq \mathcal{L}$
                \item Let $A,B \in \mathcal{L}, A \subseteq B$. We want $B\setminus A \in \mathcal{L}$.
                \begin{align*}
                    P\Big((B\setminus A) \bigcap F\Big) = P\Big((B \bigcap F)\setminus(A \bigcap F)\Big)
                    = P(B \bigcap F) - P(A \bigcap F)
                \end{align*}
                \item Let $\mathcal{L} \ni A_{i} \nearrow A.$. We want: $A \in \mathcal{L}$
                \begin{align*}
                    P(A\bigcap F) = \lim_{n \to \infty}P(A_n \bigcap F) \qquad \text{because }
                    A_n \bigcap F \nearrow A\bigcap F
                \end{align*}
            \end{enumerate}
            We've checked that $\mathcal{L}$ is a $\lambda$-system. So $\sigma(A_1) \subseteq \mathcal{L}$

        \end{proof}
    \end{claim}
    We continue the proof of theorem~\ref{theorem:2.2}:

    Then $P(\bigcap _{i=1}^{n} A_i) = \prod_{i=1}^{n} P(A_{i}) \qquad \forall 
     A_1 \in \sigma(\mathcal{A}_1), A_2 \in \mathcal{A}_{2}, \ldots ,A_n \in \mathcal{A}_{n}$

     We can use the same argument to upgrade each $\mathcal{A}_i$ in turn to $\sigma(\mathcal{A}_{i})$.
     At the end we have the product properties for all members of $\sigma(\mathcal{A}_{1}), \ldots ,
     \sigma(\mathcal{A}_{n})$
\end{proof}

\begin{corollary}
    $\mathbb{R}$-valued r.v.'s $X_1, \ldots ,X_n$ are independent iff 
    \begin{align*}
        P(\bigcap _{i=1}^{n}\{ X_{i} \le s_i \}) = \prod_{i=1}^{n}P\{ X_{i} \le s_i \} 
    \end{align*}
\end{corollary}

\DATE{Sep 17, 2024}

Today:
\begin{equation*}
    \text{Independent r.v's}
    \begin{cases} 
    \text{product measure}\\ 
    \text{convolutions}   
    \end{cases}
\end{equation*}

\subsubsection{product measures}
\begin{definition}
    Suppose $(X_1, \mathcal{A}_1, \mu_1), \ldots ,(X_n,\mathcal{A}_n,\mu_n)$ are $\sigma$-finite measure spaces.
    The $\underline{\text{product}}$ measure space $(X,\mathcal{A},\mu)$ is defined as follows:
    \begin{align*}
        &X = \prod_{i=1}^{n} X_{i} = \text{ the Cartesian product, } \mathcal{A} = \text{ product }\sigma-algebra
        = \otimes_{i=1}^{n}\mathcal{A}_i = \sigma \{ A_1\times \cdot \times A_n: A_i \in \mathcal{A}_i \}\\
        &\mu = \text{product measure} = \otimes_{i=1}^{n}\mu_i = \text{ by def the unique measure }\mu \text{ on }
        \mathcal{A} \text{ such that }\\
        &\hspace*{15mm} \mu(A_1 \times \cdot  \times A_n) = \prod_{i=1}^{n}\mu_i(A_i) 
        \quad \forall A_1 \in \mathcal{A}_1, \ldots ,
        A_n \in \mathcal{A}_n 
    \end{align*}
\end{definition}

\begin{theorem}[Tonelli-Fubini Theorem]
    \,

    $(n=2)$: $\int_{X \times Y} f(x,y) \mu \otimes v (dx,dy) = \int_{Y}[\int_{X}f(x,y)\, \mu(dx)]v(dy)$
\end{theorem}

Suppose each $X_{i}$ is a metric space w.r.t $\mathcal{B}_{X_{i}}$; also $X$ is a 
metric space w.r.t $\mathcal{B}_{X}$. Relationship of $\otimes_{i=1}^{n}\mathcal{B}_{X_{i}}$ $\&$ $\mathcal{B}_{X}$.

Take $n=2$: $\mathcal{B}_{X} \otimes \mathcal{B}_{Y} = \sigma \{ A \times B: A \in \mathcal{B}_{X},
B \in \mathcal{B}_{Y} \} = \sigma\{\underbrace{A \times B}_{\text{open set in } X \times Y}:A
 \subseteq X \text{ open, }B \subseteq Y \text{ open } \}$ 

\begin{definition}
    Separable metric space has a countable dense subset.
\end{definition}

\begin{example}
    \,

    \begin{enumerate}
        \item $\mathbb{R}^{d}$
        \item $C[0,1]$
        \item $C([0,\infty])$: here the metric $d(f,g) = \sup_{0 \le x < \infty}|f(x) - g(x)|$ makes not separable!\\
        But $d(f,g) = \sum_{n=1}^{\infty}2^{-n}(\sup_{0\le x\le n}|f(x) - g(x)| \land 1)$
    \end{enumerate}
\end{example}

\begin{theorem}[Proposition 1.5 in Folland]
    \,

    $\underline{\text{Fact}}$: If $X,Y$ are separable metric spaces, then $\mathcal{B}_{X} \otimes \mathcal{B}_{Y}
    = \mathcal{B}_{X\times Y}$
\end{theorem}

\begin{remark}
    reference: Richard M. Dudley: Real Analysis and Probability, Prop 4.1.7
\end{remark}


\begin{definition}
    Suppose $X_1, \ldots ,X_n$ are r.v.'s on $(\Omega, \mathcal{F}, P)$. Let $\mu_i(B) = P(X_{i} \in B),
\, B \in \mathcal{B}_{\mathbb{R}}$ be the distribution (marginal distribution of $X_{i}$) of $X_{i}$

$ X = (X_1, \ldots ,X_n)$ is an $\mathbb{R}^{n}$-valued random variable and its distribution (joint distribution
of $X_1, \ldots ,X_n$)
is a probability measure $\mu$
on $\mathbb{R}^{n}$.
\end{definition}


\begin{theorem}
    $X_1, \ldots ,X_n \text{ independet }\iff \mu = \otimes_{i=1}^{n}\mu_i$
\end{theorem}
\begin{proof}
    \,

    \begin{enumerate}
        \item $\Longrightarrow$:
        Let $A_1 \times \cdots \times A_n \in \mathcal{B}_{\mathbb{R}^{n}}$.
        \begin{align*}
            \mu(A_1 \times \cdots \times A_n) = P\{ (X_1, \ldots ,X_n) \in A_1 \times \cdots\times A_n \}\\
            = P\{ X_1 \in A_1, \ldots ,X_n \in A_n \} 
                = \prod_{i=1}^{n}P(X_{i} \in A_i) 
                = \prod_{i=1}^{n}\mu_i(A_i). \\
            \pi-\lambda \text{ thm} \Longrightarrow \mu = \otimes_{i=1}^{n}\mu_i
        \end{align*}
        \item $\Longrightarrow$ Similar
    \end{enumerate}
\end{proof}

\begin{corollary}
    If $E|f_i(X_{i})| < \infty$ for $i=1, \ldots n$, $X_1, \ldots ,X_n$ independent, then
    \begin{align*}
        E\Big[\prod_{i=1}^{n}f_i(x_i)  \Big] = \prod_{i=1}^{n}E\Big[f_i(X_{i})\Big] 
    \end{align*}
\end{corollary}
\begin{proof}
    Note that when $X_1, \ldots ,X_n$ are independent, then $f(X_1), \ldots ,f(X_n)$ are independent.

    Take $n=2$. Let $\mu_i = P \circ X_{i}^{-1}$
    \begin{align*}
        E[f_1(X_{i})f_2(X_{2})] &= \int_{\mathbb{R}^{2}}f_1(x_1)f_2(x_2)(\mu_1 \otimes \mu_2)(dx_1 dx_2)\\
        &= \int_{R}\mu_2(dx_2)\int_{\mathbb{R}}\mu_1(dx_1)f_1(x_1)f_2(x_2)\\
        &= \int _{\mathbb{R}}u_2(dx_2)f_2(x_2)\int_{\mathbb{R}}u_1(x_1)f_1(x_1)\\
        &= E\Big[f_1(X_1) \Big]E\Big[f_2(X_2)\Big]
    \end{align*}
\end{proof}
\begin{remark}
    It's OK to mix notation: if $X \independent Y$, then 
    \begin{align*}
        E\Big[g(X,Y)\Big] = \int g \, d \mu\otimes \nu = \int \nu(dy)\int \mu(dx) g(x,y)\\
        = \int \nu(dy)\mathbb{E}\Big[g(X,y)\Big]
    \end{align*}
\end{remark}

\begin{corollary}
    Let $X = (X_1, \ldots ,X_n)$ have PDF $f$ on $\mathbb{R}^{n}$, and let $f_i$ be PDF of
    $X_{i}$ for $i=1, \ldots ,n$. Then
    \begin{align*}
        X_1, \ldots ,X_{n} \text{ are independent} \iff f(x_1, \ldots ,x_n) = \prod_{i=1}^{n}f_i(x_i)
        \quad \text{for Lebesgue almost every} (x_1, \ldots ,x_n) \in \mathbb{R} 
    \end{align*}
\end{corollary}

\begin{definition}[convolutions]
    Let $\mu, v$ be Borel probability measure on $\mathbb{R}$. Their $\underline{\text{
        convolution
    }}$ is 
    \begin{align*}
        \mu * \nu(B) = \int_{\mathbb{R}} \mu(B-x)\nu(dx), \quad B \in \mathcal{B}_{\mathbb{R}}
    \end{align*}
\end{definition}

Why is $\mu(B-x)$ is measurable?

$\mu(B-x) = \int_{\mathbb{R}}\1_{B-x}(y)\mu(dy) = \int_{\mathbb{R}}
\underbrace{\1_{B}(x+y)}_\text{jointly measurable function}(x,y)\mu(dy)$

Fubini $\Longrightarrow$ the intepretation over $y$ leaves a measurable function of the variable $x$.

\vspace{10mm}
We consider the probability meaning of $\mu*v$:
\begin{align*}
    \mu * \nu(B) &= \int_{\mathbb{R}}\Big[\int_{\mathbb{R}}\1_{B}(x+y)\mu(dx) \Big]\nu(dy)\\
    &= \int_{\mathbb{R}^{2}}\1_{B}(x+y)(\mu \otimes \nu)(dx,dy)\\
    &= \mathbb{E}\Big[\1_{B}(X+Y)\Big]\\
    &= P(X+Y \in B)
\end{align*}
Let $X \independent Y$, $X \sim  \mu, Y \sim  \nu$. Then we have $(X,Y) \sim  \mu\otimes \nu$

\begin{theorem}
    $X \independent Y$, $X \sim  \mu$, $Y \sim \mu $ $\Longrightarrow X+Y \sim  \mu\times \nu$
\end{theorem}

What happened 

Suppose $\mu$ has PDF $f$, $\nu$ has PDF $g$. Find 
\begin{align*}
    \mu * \nu(A) = \int_{\mathbb{R}^{2}}\1_{A}(x+y)\mu(dx)\nu(dy)\\
    = \int_{\mathbb{R}^{2}}\1_{A}(x+y)f(x)g(y), dxdy\\
    = \int_{\mathbb{R}}dxf(x) \int_{\mathbb{R}}\1_{A}(x+y)g(y)\, dy\\
    = \int_{\mathbb{R}}dxf(x) \int_{\mathbb{R}}\1_{A}(y)(y-x)\,dy\\
    = \int_{\mathbb{R}}dy\1_{A}(y) \int_{\mathbb{R}}dx f(x)g(y-x)
\end{align*}
By definition $f*g(y)$ we see that is the PDF of $\mu * v$

\begin{example}
    Gaussian density: $f(x_1) = \frac{1}{\sqrt{2\pi\sigma_1^{2}}}e^{-\frac{(x-m_1)}{
        2\sigma_1^{2}
    }}$ and $f(x_2) = \frac{1}{\sqrt{2\pi\sigma_2^{2}}} e^{-\frac{(x-m_2)^{2}}{2\sigma_2^{2}}}$.

    We have 
    \begin{align*}
        (f_1 * f_2)(x) = \frac{1}{\sqrt{2\pi (\sigma_1^{2} + \sigma_2^{2})}}
        e^{-\frac{(x - m_1 - m_2)^{2}}{2(\sigma_1^{2}+ \sigma_2^{2})}}
    \end{align*}
\end{example}

\DATE{Sep 19, 2024}

\subsubsection{Construction of probability spaces with desired independent r.v.'s}
2.1.4 section in Durett

Finite case: Given $\mu_1, \ldots ,\mu_n$ Borel probability measure on $\mathbb{R}$.

$\underline{\text{Want:}}$ independent r.v,'s $X_1, \ldots ,X_{n}$ with $X_{i} \sim \mu_1$

Take $\Omega = \mathbb{R}^{n} = \{ \omega = (x_1, \ldots ,x_n): x_i \in \mathbb{R} \}$.
$\mathcal{F} = \mathcal{B}_{\mathbb{R}} = \mathcal{B}_{\mathbb{R}}^{\otimes n}$ (
    $X_{i}$ has probability distribution $\mu_i$
),
$P = \otimes_{i=1}^{n}\mu_i$, $X_{i}(\omega) = x_i$ ("coordinate r.v.'s coordinate projections").

Given $B_1, \ldots ,B_n \in \mathcal{B}_{\mathbb{R}}$
\begin{align*}
    P(X_{1} \in B_1, \ldots ,X_n \in B_n) &= P\{ \omega \in \Omega: X_{i}(\omega) \in B_1, \ldots ,
    X_n(\omega) \in B_n \}\\
    &= (\otimes_{i=1}^{n}\mu_i)\{ (x_1, \ldots ,x_n) \in \mathbb{R}^{n}: x_1 \in B_1, \ldots ,
    x_n \in B_n\}\\
    &= (\otimes_{i=1}^{n}\mu_i)(\prod_{i=1}^{n}B_i )\\
    &= \prod_{i=1}^{n}\mu_i(B_i)\\
    &= \prod_{i=1}^{n} P(X_{i} \in B_i) \quad \text{ by }~\ref{eq:1}
\end{align*}
Intermediate step: pick $j$, take $B_i = \mathbb{R}$ for $i \neq j$, substitute with the calculation:
\begin{equation} \label{eq:1}
    \begin{aligned}
        P(X_{j} \in B_j) = \prod_{i=1}^{n}  \mu_i(B_i) = \mu_{j}(B_j) \\
        \Longrightarrow X_j \sim \mu_j
    \end{aligned}
\end{equation}
This is all works if we replace and $\mathbb{R}, \mathcal{B}_{\mathbb{R}}$ with arbitrary measurable
spaces $(S_i, \mathcal{A}_{i})$. The choice of $(\Omega, \mathcal{F}, P)$ is not unique at all!

\begin{definition}[Infinite case]
    A $\underline{\text{stochastic process}}$ is an dexed collection 
    $\{ X_{\alpha}: \alpha \in \mathcal{J} \}$ of r.v.'s all defined on the $\underline{\text{same}}$
    $(\Omega, \mathcal{F}, P)$.
\end{definition}

\begin{theorem}[Kolmogorov's Extension Theorem]
    (for index set $\mathbb{Z}_{\ge 0}$) Assume that $\forall n \ge 1$, we have a probability measure
    $\vu_{n}$ on $(\mathbb{R}^{n}, \mathcal{B}_{\mathbb{R}^{n}})$ and these measures are 
    consistent: $\forall  B \in \mathcal{B}_{\mathbb{R}^{n}}: \vu_{n+1} = 
    \vu_{n}(B)$

    Let $\Omega = \mathbb{R}^{\mathbb{Z} \ge 0} = \{ \omega = (x_i)_{i=1}^{\infty}: 
    \text{ each } x_i \in \mathbb{R} \}$, $\mathcal{F}$ = product $\sigma$-algebra = 
    $\sigma \{ A_1 \times \cdots \times A_n \times \mathbb{R}
    \times \mathbb{R} \cdots: n \in \mathbb{Z}_{> 0}, A_1, 
    \ldots ,A_n \in \mathcal{B}_{\mathbb{R}} \} =$ $\sigma$-algebra generated by the projection mapping
    $X_{i}(\omega) =x_i, \; i \in \mathbb{Z}_{>0},\,=$ smallest $\sigma$-algebra on $\Omega$ under
    which each $X_{i}: \Omega \to \mathbb{R}$ is measurable.

    Then $\exists $ unique probability measure $P$ on $\Omega$ such that $P \{ 
        \omega \in \Omega: (X_{1}(\omega), \ldots ,X_n(\omega) \in B) = \vu_{n}(B) \quad
        \forall n \in \mathbb{Z}_{>0}, B \in \mathcal{B}_{\mathbb{R}^{n}}
     \}$
\end{theorem}

\begin{theorem}[Kolmogorov Extension theorem process version]
    Given consistent finite-dim distribution $\{ \vu_{n} \}_{n \ge 1}$ on $\mathbb{R}^{n} \quad
    \forall n$, $\exists $ a stochastic process $(X_{k})_{k \in \mathbb{Z}_{>0}}$ with marginal
    $(X_1, \ldots ,X_n) \sim \vu_{n}$
\end{theorem}
\begin{proof}
    Take the coordinate process from the previous theorem.
\end{proof}

$\underline{\text{Generalizations:}}$ 
\begin{enumerate}
    \item Instead of $\mathbb{R}$, we can take any Borel subsets of 
    $\underbrace{\text{complete separable metric spaces.}}_{\text{"Polish spaces"}}$.
    \item The index set can be totally arbitrary. [cf. Dudley's book]
\end{enumerate}

To produce a process $(X_k)_{k \in \mathbb{Z}_{>0}}$ of independent r.v.'s with $X_k \sim  \mu_k$,
take $\vu_{n} = \mu_1 \otimes\cdots\otimes \mu_n$ in K's extension theorem.

\begin{definition}
    An IID process is a process of independent $\underline{\text{identically distributed }}$ r.v.'s.
\end{definition}


\subsection{Strong Law Large Number (2.4 in Durett)}

Two big goals for IID process $\{ X_k \}_{k \in \mathbb{Z} > 0}$
\begin{theorem}
    If $\mathbb{E}|X_1| < \infty$, then $S_n = X_1+ \cdots +X_n$ satisfies
    \begin{align*}
        \frac{S_n}{n} \longrightarrow \mathbb{E}X_1 \quad \text{w.p.}1
    \end{align*}
\end{theorem}

\begin{theorem}
    Central Limit Theorem: if $\sigma^{2} = \text{Var}(X_1) < \infty $, then 
    \begin{align*}
        P \{  \frac{S_n - n\mathbb{E}X_1}{\sigma \sqrt{n}} \le s \} \xrightarrow{n \to \infty}
        \int_{-\infty}^{s}\frac{e^{\frac{-x^{2}}{2}}}{\sqrt{2 \pi}}\,dx
    \end{align*}
\end{theorem}

\begin{definition}
    Let $\{ A_n \}$ be a sequence of events in $(\Omega, \mathcal{F}, P)$.
    \begin{align*}
        \{ A_n \text{i.o.(inifinitly often)} \} = \{ \omega \in \Omega: \omega \in A_n
        \text{ for infinityly many }n \}\\
        = \{ \omega \in \Omega: \forall m \ge 1, \exists n \ge m \text{ s.t. } \omega \in A_n \}\\
        = \bigcap _{m\ge 1}\bigcup _{n\ge m} A_n(= \bar{lim}A_n)
    \end{align*}
\end{definition}

\begin{theorem}[1st Borel-Cantelli Lemmas]
    \begin{align*}
        \sum_{n=1}^{\infty}P(A_n) < \infty \Longrightarrow P(A_n \text{ i.o.}) = 0
    \end{align*}
\end{theorem}
\begin{proof}[(1.)]
    Let $N(\omega) = \sum_{n=1}^{\infty}\1_{A_n}(\omega) = \#. $ of events that occur.
    \begin{align*}
        \mathbb{E}[N] \overset{\text{MCT}}{=} \sum_{n=1}^{\infty}E[\1_{A_n}] = \sum_{n=1}^{\infty}P(A_n) < \infty\\
        \Longrightarrow P(N=\infty) = 0
    \end{align*}
\end{proof}
\begin{proof}[(2.)]
    \begin{align*}
        P(\bigcap _{m\ge 1}\bigcup _{n\ge m} A_n) = \lim_{m \to \infty}P(\bigcup_{n\ge m}A_n)\\
        \le \lim_{n \to \infty}\sum_{n\ge m}P(A_n) = 0 \quad \text{ by convergent series tails}
    \end{align*}
\end{proof}
\begin{definition}
    (Suppose all defined on the same probability space) $X_n \xrightarrow{a.s.} X$ if $P\{ 
        \omega \in \Omega: \lim_{n \to \infty}X_n(\omega) = X(\omega)
     \}$ = 1
\end{definition}
\begin{lemma}
    Suppose $\forall  \epsilon > 0, \, \sum_{n=1}^{\infty}P(|x_n-x| \ge \epsilon) < \infty$.

    Then $X_n \longrightarrow X \quad a.s.$.
\end{lemma}
\begin{proof}
    Pick any sequences $0 < \epsilon_{j} \searrow$
    \begin{align*}
        \text{B-C} \Longrightarrow P(\bigcap _{m\ge 1}\bigcup _{n\ge m}\{ |X_n-X| \ge \epsilon_j \}) &= 0\\
        \Longrightarrow &1=  P(\bigcup_{m\ge 1}\bigcap_{n\ge m}\{ |X_n - X| < \epsilon_j \})\\
        &= P\{ \exists m < \infty \; s.t. \; n\ge m \Longrightarrow
        |X_n-X| < \epsilon_j \}\\
        1 &= P(\bigcap _{j=1}^{\infty}\bigcup _{m\ge 1}\bigcap_{n\ge m}\{ |X_n-X| < \epsilon_j \})\\
        &= P\{ \forall j \;\exists m,\, m,n \ge m \Longrightarrow|X_n-X| < \epsilon_j \}\\
        &= P\{ X_n \to X \}
    \end{align*}
\end{proof}
\begin{example}
    Suppose $\mathbb{E}|Y_n| \le 2^{-n}$. Then $Y_n \xrightarrow{a.s.}0$.
    \begin{proof}
        $\sum_{n\ge 1}P(|Y_n|\ge \epsilon) \le \sum_{n=1}^{\infty}\frac{E|Y_n|}{\epsilon}
        \le \frac{1}{\epsilon} \sum_{n=1}^{\infty}2^{-n} < \infty$
    \end{proof}
\end{example}
\begin{remark}[Markov-Chebyshev]
    Suppose r.v. $Z > \ge 0$, $a>0$:
    \begin{align*}
        P(Z \ge a) = \mathbb{E}[\1_{Z\ge a}]\le \mathbb{E}[\frac{Z}{a}\1_{Z\ge a}] \overset{Z\ge_0}{\le}
        \mathbb{E}[\frac{Z}{a}] 
    \end{align*}
\end{remark}

\begin{lemma}[Borel-Cantelli]
    $\{ A_n \}_{n\ge 1}$ a sequence of events on $(\Omega, \mathcal{F}, P)$ and $
    \sum_{n}P(A_n) < \infty \Longrightarrow P(\bigcap_{m=1}^{\infty}\bigcup_{n\ge m} A_n) = 0$
\end{lemma}

\begin{lemma}[2nd Borel-Cantelli]
    Let $\{ A_n \}_{n\ge 1}$ be independent events. Then 
    \begin{align*}
        \sum_{n}P(A_n) < \infty \Longrightarrow P(\bigcap_{n=1}^{\infty}\bigcup_{n=m}^{\infty}) = 1
    \end{align*}
\end{lemma}

\begin{proof}
    Let $M < N$. 
    \begin{align*}
        P(\bigcap_{n=M}^{N}A_{n}^{c}) = \prod_{n=m}^{N} (1-P(A_n)) \le \prod_{n=M}^{N} e^{-P(A_n)}\\
        = e^{-\sum_{n=M}^{N}P(A_n)} \underset{N \to \infty}{0}\\
        \Longrightarrow P(\bigcap_{n=M}^{\infty}A_n^{c}) = 0\\
        \Longrightarrow 1= P(\bigcup_{n=M}^{\infty}A_n) = 1 \quad \text{ true } \forall M
    \end{align*}
    So 
    \begin{equation}
        P(\bigcap_{M=1}^{\infty}\bigcup_{n=M}^{\infty}) = 1
    \end{equation}
\end{proof}

\begin{example}
    Roll a fair dice $\infty$ often. $X_n = \text{ outcome of }n \text{-th roll}$.
    \begin{align*}
        \sum_{n=1}^{\infty}P(X_n = 6) = \sum_{n=1}^{\infty}\frac{1}{6} = \infty\\
        \overset{\text{2nd} B-C}{\Longrightarrow} P(X_n = 6 \text{ i.o.})  = 1
    \end{align*}
\end{example}

\begin{definition}
    $X_n \overset{a.s.}{X}$ if $P\{ \omega \in \Omega: \lim_{n \to \infty} \}X_n(\omega) = X(\omega)$
\end{definition}

\begin{definition}
    $X_n$ converges to $X$ $\underline{\text{in probability }} (X_n \overset{P}{\to} X)$ 
    if $\forall \epsilon > 0$
    \begin{align*}
        P(|X_n - X| \ge \epsilon) \underset{n\to \infty}{0}
    \end{align*}
\end{definition}

\begin{remark}[Observation]
    $X_n \overset{a.s.}{\longrightarrow} X \Longrightarrow X_n \overset{P}{\longrightarrow}$
    \begin{align*}
        1 = P(X_n \longrightarrow X) = P(\bigcap_{k=1}^{\infty}\bigcup _{m=1}^{\infty}\bigcap_{n=m}^{\infty}
        \{ |X_n - X| \le \frac{1}{k} \})\\
        \le P(\bigcup _{m=1}^{\infty}\bigcap _{n=m}^{\infty}\{ |X_n-X| \le \frac{1}{k} \}) \quad \forall \, k \in \mathbb{Z}_{> 0}
        \quad \text{(Notice that this increases with )}m \\
        = \lim_{m \to \infty}P(\bigcap_{n=m}^{\infty}\{ |X_n-X| \le \frac{1}{k} \}) \\
        \le \underbrace{\lim_{m \to \infty} P(|X_m - X| \le \frac{1}{k})}_{\text{this limit} = 1 \; \forall k}
    \end{align*}
\end{remark}

\begin{theorem}
    \,

    $X_n \overset{P}{\longrightarrow} X \Longrightarrow $ 
    Every subsequence $\{ X_{n_{(m)}} \}_{m=1}^{\infty}$ has 
    a further subsequence $X_{n(m_{k})} \overset{a.s.}{\underset{k\to \infty}{\longrightarrow}} X$
\end{theorem}
\begin{proof}
    \begin{itemize}
        \item $(\Longrightarrow)$
        Assumption is : $\forall  \epsilon > 0, P(|X_n-X| \ge \epsilon) \longrightarrow \epsilon$

        Let subseq $\{ n(m) \}_{m=1}^{\infty}$ be given. Pick $n(m_1) < n(m_2)<\cdots<n(m_{k})<\cdots$
        such that
        \begin{align*}
            P(|X_{n(m_{k})} - X| \ge 2^{-k}) < 2^{-k}\\
            \sum_{k=1}^{\infty}2^{-k} < \infty \Longrightarrow \text{ by B-C, w.p.1.}\\
            \exists k_0 = k_0(\omega) \text{ s.t. } k \ge \underbrace{k_0(\omega)}_{\text{random index}}
             \Longrightarrow 
            |X_{n(m_{k})} - X| < 2^{-k} \quad \omega \in \Omega
        \end{align*}
        \begin{remark}
            $k_0$ is random means that it is a function of $\omega$.
        \end{remark}
        \item $(\Longleftarrow)$ pf by contradiction: suppose $X_n \overset{P}{\longrightarrow}$ fails.
        Then $\exists \epsilon > 0$ s.t. $P(|X_n - X| \ge \epsilon) \longrightarrow 0$ Fails.
        So $\exists $subseq $n(m)$ and $\delta > 0$ s.t. 
        $P(|X_{n(m)} - X| \ge \epsilon) \ge \delta$. But by assumption $\exists $further subsequence
        converges a.s. $X_{n(m_{k})} \overset{\longrightarrow}{X} X \Longrightarrow
        X_{n(m_{k})} \overset{P}{\longrightarrow} X \Longrightarrow 
        P(|X_{n(m_k)} - X| \ge \epsilon) \longrightarrow 0$
    \end{itemize}
\end{proof}

\begin{theorem}[SLLN and WLLN]
    SLLN: \begin{align*}
        P(\frac{S_n}{n} \longrightarrow \mathbb{E}X_1) = 1
    \end{align*}
    where $S_n = \sum_{i=1}^{n}X_k$ IID $\{ X_k \}$
    WLLN: $\frac{S_n}{n} \overset{P}{\longrightarrow} \mathbb{E}X_1$
\end{theorem}

Here we can write $\frac{S_n - \mathbb{nE}X_1}{n} = \frac{S_n - \mathbb{E}S_n}{n}
\overset{a.s.}{0}$

\begin{lemma}[Chebyshev's Inequality]
    \begin{align*}
        P(|X-\mathbb{E}X| \ge a)  =  P(|X-\mathbb{E}X|^{2} \ge a^{2})\\
        \le \frac{\mathbb{E}[|X-\mathbb{E}X|^{2}]}{a^{2}}\\
        = \frac{\operatorname{Var}(X)}{a^{2}}
    \end{align*}
    \begin{remark}
        Why do we wanna use the square? Because $L^{2}$ is the Hilbert space but $L^{1}$ is just a 
        Banach space.
    \end{remark}
\end{lemma}

\begin{definition}
    $X$ and $Y$ are $\underline{\text{uncorrelated}}$ is $0 = \operatorname{Cov}(X,Y)
    = \mathbb{E}[(X-\mathbb{E}X)(Y-\mathbb{E}Y)] = E[XY] - \mathbb{E}X \mathbb{E}Y$.
\end{definition}

We know that $X \independent Y \Longrightarrow X$ and Y uncorrelated.

Computation:
\begin{align*}
    \operatorname{Var}(X+Y) = \mathbb{E}[(X+Y)^{2}] - (\mathbb{E}X + \mathbb{E}Y)^{2}\\
    = \mathbb{E}(X^{2}) + 2\mathbb{E}(XY) + \mathbb{E}(Y^{2}) - (\mathbb{E}X)^{2} - 2\mathbb{E} X \mathbb{E}Y 
    - (\mathbb{E}Y)^{2} = \operatorname{Var}(X) + \operatorname{Var}(Y) + 2\operatorname{Cov}(X,Y)
\end{align*}

\begin{theorem}[SLLN]
    Let $\{ X_k \}_{k=1}^{\infty}$ be pairwise independent, identically distributed, integrable.
    Let $S_n = \sum_{k=1}^{n}X_k$. Then $P\{ \frac{S_n}{n} \longrightarrow EX_1 \} = 1$
\end{theorem}
\begin{proof}
    \,

    Truncation: define $Y_k = X_k \cdot \1_{\{ |X_k| \le k \}}$. Let $T_n = \sum_{k=1}^{n} Y_k$
    \begin{lemma}[(a)]
        The theorem will follow from showing that 
        \begin{align*}
            \frac{T_n}{n} \overset{a.s.}{\longrightarrow}EX_1
        \end{align*}
    \end{lemma}
    \begin{proof}[proof of lemma (a.)]
        \begin{align*}
            \sum_{k=1}^{\infty}P(X_k \neq Y_k) = \sum_{k=1}^{\infty}P(|X_k|>k)\\
            = \sum_{k=1}^{\infty} \int_{k-1}^{k} P(|X_k| > k) \mathrm{d}t \\
            \le \sum_{k=1}^{\infty} \int_{k-1}^{k}P(|X_1| . t)\, dt \\
            = \int_{0}^{\infty}P(|X_1| > t)\,dt \\
            = E|X_1| \le \infty
        \end{align*}
        Notice for $\int_{0}^{\infty}P(|X_1| > t)\,dt$:
        \begin{align*}
            \int_{0}^{\infty}E\1_{|X_1| > t}\,dt = E\int_{0}^{\infty}\1_{|X_1| > t}\,dt \\
            = E \int_{0}^{|X_1|}\,dt = E|X_1|
        \end{align*}
        $\text{B-C} \Longrightarrow \exists \text{ random }k_0$ s.t.
        \begin{align*}
            k > k_0 \Longrightarrow X_k = Y_k
        \end{align*}
        Suppose $\frac{T_n}{n} \longrightarrow EX_1$ a.s.
        \begin{align*}
            \frac{S_n}{n} = \frac{T_n}{n} + \frac{S_n - T_n}{n}\\
            \Longrightarrow \frac{|S_n - T_n|}{n} \le \frac{1}{n} \sum_{k=1}^{n}|X_k-Y_k|\\
            \le \frac{1}{n}\sum_{k=1}^{k_0(\omega)} |X_k(\omega)-Y_k(\omega)|
            \underset{n \longrightarrow \infty}{\longrightarrow} 0
        \end{align*}
        \DATE{Sep 26, 2024}
        Last time we observed $EX = \int_{0}^{\infty}P(X > s)  \mathrm{d}s$ for $X \ge 0$.

        This generalizes: let $X \ge 0,\, h:[0,\infty) \to \mathbb{R}$ nondecreasing, $h(0) = 0,\, 
        h \in C^{1}$:
        \begin{align}
            E[h(x)] = E[h(x) - h(0)] = E[\int_{0}^{X}h'(x)  \mathrm{d}x] = 
            E[\int_{0}^{\infty}h'(x)\1_{X > 0}  \mathrm{d}x] \\
            = \int_{0}^{\infty}h'(x)P(X > 0)  \mathrm{d}x
        \end{align}
    \end{proof}
    \begin{lemma}[b.]
        $\sum_{k=1}^{\infty} \frac{\operatorname{Var}(Y_k)}{k^{2}} \le 4E|X_1| < \infty$
    \end{lemma}
    \begin{proof}
        \begin{align}
            \operatorname{Var}(Y_k) \le E(Y_k^{2}) = \int_{0}^{\infty}2y \underbrace{P(|Y_k| > y)}_{
                y>k: = 0; \; y < k: \le P(|X_k| > y)
            }  \mathrm{d}y \\
            \le \int_{0}^{k}2y P(|X_1| > y)  \mathrm{d}y 
        \end{align}
        Then we have
        \begin{align}
            \sum_{k=1}^{\infty}\frac{\operatorname{Var}(Y_k)}{k^{2}} \le \sum_{k=1}^{\infty}
            \frac{1}{k^{2}}\int_{0}^{\infty}\1_{y<k}2yP(|X_1| > y)  \mathrm{d}y \\
            = \int_{0}^{\infty}
            \underbrace{(\sum_{k=1}^{\infty}\frac{1}{k^{2}}\1_{y<k}2y)}_{A(y)}
            P(|X_1|>y)  \mathrm{d}y \\
            \overset{\text{lemma c}}{\le} 4\int_{0}^{\infty}P(|X_1| > y)  \mathrm{d}y = 4E|X_1|
        \end{align}
    \end{proof}
    \begin{lemma}[c.]
        $A(y) \le 4 \quad \forall \,y\ge 0$
    \end{lemma}
    \begin{proof}[c.]
        \begin{align}
            \sum_{k \ge 0}\frac{1}{k^{2}} \le \int_{m-1}^{\infty}\frac{1}{x^{2}}  \mathrm{d}x\\
            = \frac{1}{m-1} \quad \text{ for }m \ge 2\text{, }m \in \mathbb{Z}
        \end{align}
        For $y \ge 1:$
        \begin{align}
            2y \sum_{k > y}\frac{1}{k^{2}} = 2y \sum_{k\ge \lfloor y \rfloor + 1}\frac{1}{k^{2}}\\
            \le \frac{2y}{\lfloor y \rfloor} \\
            \le 2 \frac{\lfloor y \rfloor + 1}{\lfloor y \rfloor} \\
            = 2(1+\frac{1}{\lfloor y \rfloor}) \\
            \le 4 
        \end{align}
        For $0 < y < 1$:
        \begin{align}
            2y\sum_{k > y}\frac{1}{k^{2}} = 2y \sum_{k=1}^{\infty}\frac{1}{k^{2}} \\
            \le 2(1+ \sum_{k=2}^{\infty}\frac{1}{k^{2}}) \le 4
        \end{align}
        \begin{remark}
            For $x \in \mathbb{R},\, \lfloor  x  \rfloor = \max\{ n \in \mathbb{Z}: n \le x \}$
        \end{remark}
    \end{proof}
    Let $\alpha > 1$ (real), $k(n) = \lfloor \alpha ^{n} \rfloor$
    \begin{align*}
        \sum_{n=1}^{\infty}P(|T_{k(n)} - ET_{k(n)}| \ge \epsilon k(n)) \le 
        \sum_{n=1}^{\infty} \frac{\operatorname{Var}(T_{k(n)})}{\epsilon ^{2}k(n)^{2}}\\
        = \epsilon^{-2}\sum_{n=1}^{\infty} \frac{1}{k(n)^{2}}\sum_{m=1}^{k(n)}
        \operatorname{Var}(Y_m) \\
        = \epsilon ^{-2}\sum_{m=1}^{\infty}\operatorname{Var}(Y_m) 
        \underbrace{\sum_{n: k(n) \ge m}\frac{1}{k(n)^{2}}}_{[*]} \\
        \le \frac{4 \epsilon ^{-2}}{1-\alpha ^{-2}}\sum_{m=1}^{\infty}
        \frac{\operatorname{Var}(Y_m)}{m^{2}} < \infty \quad \text{by lemma} (b)
    \end{align*}
    And we have 
    \begin{align*}
        [*] = \sum_{n: k(n)\ge m}\frac{1}{k(n)^{2}} \\
        = \sum_{n: \alpha ^{n} \ge m} \frac{1}{\lfloor \alpha ^{n} \rfloor ^{2}} \\
        \le 4 \sum_{n: \alpha ^{n} \ge m}\alpha ^{-2n} \\
        \le 4 \frac{m^{-2}}{1-\alpha ^{-2}}
    \end{align*}
    \begin{claim}
        $\lfloor \alpha ^{n} \rfloor \ge \frac{\alpha ^{n}}{2}$ because $\alpha > 1$
    \end{claim}
    By B-C: 
    \begin{align*}
        \frac{T_{k(n)}}{k(n)} - \frac{ET_{k(n)}}{k(n)} \overset{a.s.}{\longrightarrow} 0
    \end{align*}
    Check the $\frac{ET_{k(n)}}{k(n)}$:
    \begin{align*}
        EY_{k} = E[X_1 \cdot \1_{|X_1| \le k}] \underset{k \longrightarrow \infty}{\longrightarrow} EX_1
    \end{align*}
    by DCT and assumption $E|X_1| < \infty$

    Thus 
    \begin{align*}
        \frac{ET_{k(n)}}{k(n)} = \frac{1}{k(n)}\sum_{m=1}^{k(n)}E(Y_m) \underset{n \longrightarrow \infty}
        {\longrightarrow}EX_1
    \end{align*}
    \begin{remark}
        Cesaro convergence: $a_k \underset{k \longrightarrow \infty}{a} \Longrightarrow
        \frac{1}{n}\sum_{k=1}^{n}a_k \underset{n\to\infty}{\longrightarrow} a$
    \end{remark}
    Now we have: 
    \begin{align*}
        \frac{T_{k(n)}}{k(n)} \overset{a.s.}{\longrightarrow} EX_1
    \end{align*}
    It's enough to prove the theorem for $X_k \ge 0$!

    Because then 
    \begin{align*}
        \frac{S_n}{n} = \frac{1}{n}\sum_{k=1}^{n}X_k \\
        = \frac{1}{n}\sum_{k=1}^{n}X_{k}^{+} - \frac{1}{n}\sum_{k=1}^{n}X_{k}^{-}\\
        \longrightarrow E(X_1^{+}) - E(X_1^{-}) = E(X_1)
    \end{align*}
    Now assume $X_k \ge 0$. Then also $Y_k \ge 0$.

    Let $k(n) \le m \le k(n+1)$. $T_{k(n)} \le T_m \le T_{k(n+1)}$
    \begin{align*}
        \Longrightarrow \frac{T_{k(n)}}{k(n+1)} \le \frac{T_m}{m}  \le \frac{T_{k(n+1)}}{k(n)}\\
        \Longrightarrow \frac{k(n)}{k(n+1)} \cdot \frac{T_{k(n)}}{k(n)} \\
        \le \frac{T_m}{m} \\
        \le \frac{k(n+1)}{k(n)} \cdot \frac{T_{k(n+1)}}{k(n+1)}
    \end{align*}
    Now we have $P\{ \frac{EX_1}{\alpha} \le 
        \underline{\lim}\frac{T_m}{m} \le \overline{\lim}\frac{T_m}{m} \}
        \le \alpha EX_1
        $ = 1

    Last step: $\alpha \searrow 1$ along some sequence. Then we get:
    \begin{align*}
        P \{ EX_1 \le \underline{\lim}_{m \to \infty} \le 
        \overline{\lim}_{m \to \infty} \frac{T_m}{m} \le EX_1 \} = 1
    \end{align*}
\end{proof}

Application: Empirical distribution function:
\begin{align*}
    F_n(x) = \frac{1}{n}\sum_{k=1}^{n} \1_{X_k \le x} \overset{a.s.}{\underset{n \to \infty}{\longrightarrow}} 
    E[\1_{X_1 \le x}]\\
    = P(X_1 \le x) = F(x)
\end{align*}

\begin{theorem}[Glivenko-Cantelli theorem]
    For iid real-valued r.v.'s,
    \begin{align*}
        \lim_{n \to \infty}\sup_{x \in \mathbb{R}}|F_n(x) - F(x)| = 0 \quad a.s.
    \end{align*}
\end{theorem}

Notice let $\epsilon > 0,\, x < y: \quad F(y)-F(x) < \epsilon$, suppose we know:
$|F(x) - G(x)| < \epsilon, \; |F(y) - G(y)| < \epsilon$.

For $z \in (x,y): \; G(z) - F(z) \le G(y) - F(x) = G(y) - F(y) + F(y) - F(x) < 2\epsilon$

\subsection{}[2.5]
Let $\{ X_k \}_{k=1}^{\infty}$ be r.v.'s on $(\omega, \mathcal{F}, P)$.
\begin{align*}
    \mathcal{F}_{n} = \sigma\{ X_1, \ldots ,X_n \},\, 
    \mathcal{F}_n' = \sigma \{ X_n, \ldots ,X_{n+1}, \ldots  \}
\end{align*}
We call $\mathcal{J} = \bigcap_{n=1}^{\infty}\mathcal{F}_{n}'$ be tail $\sigma$-algebra.

\begin{example}
    \begin{enumerate}
        \item $\overline{\lim}X_n$ is $\mathcal{T}$-measurable.
        \begin{align*}
            \{ \overline{\lim}X_n \ge a \} = \bigcap_{k=1}^{\infty}\{ X_n >
            a - \frac{1}{k} \;i.o. \}\\
            \bigcap_{k=1}^{\infty}\bigcap_{m=1}^{\infty}\bigcup_{n \ge m} \{ X_n > a - \frac{1}{k} \}\\
            = \bigcap_{k=1}^{\infty}\bigcap_{m=l}^{\infty}\bigcup_{n \ge m} \{ X_n > a - \frac{1}{k} \}\\
            \in \mathcal{F}_{l}' \quad \text{TRUE} \forall  l \in \mathbb{Z}_{>0} \text{ (in) }\mathcal{T}
        \end{align*}
    \end{enumerate}
    Observation: $B$
\end{example}


\DATE{Oct 1, 2024}

Now how about $\bar{S} = \overline{\lim_{n \to \infty}}S_n$?

Suppose $X_1$ is something, $X_2 = X_3 = \cdots = X_n = \cdots = 0$
\begin{align*}
    S_n = X_1 \Longrightarrow \bar{S} = X_1 \text{ not }\mathcal{T} \text{-measurable, unless}
\end{align*}

$X_k = 0, \; \forall k \Longrightarrow \bar{S} = 0$ is $\mathcal{T}$-measurable.

Assume each $X_k\,\mathbb{R}$-valued.
What about the event $\{ \lim_{n \to \infty} \text{ exists in }\mathbb{R} \}$?

This is tail measurable:
\begin{proof} 
    \begin{align*}
        \bigcap_{k=1}^{\infty} \bigcup_{N=1}^{\infty} \bigcap_{m,n \ge N}
    \{ |S_m - S_n| < \frac{1}{k} \}\\
    \bigcap_{k=1}^{\infty} \bigcup_{N\ge l}^{\infty} \bigcap_{m,n \ge N}
    \{ |S_m - S_n| < \frac{1}{k} \}\\
    \iff \{ |\sum_{i=N}^{m}X_{i} - \sum_{i=N}^{n}X_{i}| < \frac{1}{k} \} \in \mathcal{F}_{l}^{'} \quad \forall l
    \end{align*}
\end{proof}

Now we have $\eta_{x}: x \in \mathbb{Z}^{2}$. Define $\mathcal{F}_{n} = \sigma 
\{ \eta_{x}: x \in [-n,n]^{2} \bigcap \mathbb{Z}^{2} \}$. 
And $\mathcal{F}_{n}^{'} = \sigma \{ \eta_{x}: x \notin [-n,n]^{2} \}$, $\mathcal{T} = \bigcap_{n \ge 1}
\mathcal{F}_{n}'$

\begin{theorem}[Kolmogorov 0-1 law]
    Let $\{ X_k \}$ be independent. Then every $A \in \mathcal{T}$ satisfies $P(A) \in \{ 0,1 \}$.
\end{theorem}
\begin{proof}
    From the assumption and a $\pi-\lambda$ argument, $F_{n}$ and $\mathcal{F}_{n+1}^{'}$ are 
    independent $\sigma$-algebras. (Thm.2.1.9), so if $A \in \mathcal{F}_{n}$ and $B \in \mathcal{T}$ 
    are independent because the $\mathcal{T} \subseteq F_{n+1}^{'}$. 

    Hence $\bigcup _{n \ge 1}\mathcal{F}_{n}$ and $\mathcal{T}$ are independent $\pi$-systems.

    By a lemma we proved, $\sigma(\bigcup_{n\ge 1}\mathcal{F}_n)$ is independent of $\sigma(\mathcal{T})
     = \mathcal{T}$. Let $ A \in \mathcal{T}$. $T \subseteq  \mathcal{F}_{n}' = \sigma\{ X_n, \ldots , \}$
     We have 
     \begin{align*}
        \sigma(\bigcup _{n \ge 1}\mathcal{F}_{n}) \Longrightarrow A \in \sigma(\bigcup n\ge 1).
     \end{align*}
     So $A$ is independent of itself! Lence $P(A) = P(A \bigcap A) = (P(A))^{2} 
     \Longrightarrow P(A) \in \{ 0,1 \}$
\end{proof}

\begin{example}
    An $X_n$ independent. Then we know that $\{ \bar{X} \ge 0 \} \in \{ 0,1 \} \forall a\in \mathbb{R}$.

    Cliam: $\exists c \in [-\infty,\infty]$ s.t.
    $F(X = c) = 1$
    \begin{proof}
        Case 1: $P(\bar{X} > a) = 0 \quad a \in \mathbb{R}$.
        \begin{align*}
            P(\bar{X} < \infty = P(\bigcap _{k=1}^{\infty})) = 1
        \end{align*}
        Case 2: $\exists  a\in \mathbb{R}$ s.t. $P(\bar{X} > a) = 1$.
        Lt $C = \sup\{a \in \mathbb{R}: P(\bar{X} ) > 0  \}$
        \yee{To update it}
    \end{proof}
\end{example}

\section{Weak convergence}
Suppose we want to define what is it means for propabilityu on $\mathbb{R}   $ to converge.

Possible def: $\mu_{n} \to $ if $\mu_{n}(A) \underset{n \to \infty}{\longrightarrow} \quad \forall  A \in B _{\mathbb{R}}$
\begin{example}
    \begin{enumerate}
        \item $\mu_{n} = S_{\frac{1}{n}}, \frac{1}{n} \underset{n \to \infty}{\longrightarrow}$ 
        so we might want $S_{\frac{1}{n} } \underset{n \to \infty}{\longrightarrow} S_0$. 
        $[S_{x}(A)] = \1_{A}(x)$, but $S_{\frac{1}{n}}(\{ 0 \}) = 0$ and $S_{0}(\{ 0 \}) = 1$.
    \end{enumerate}
\end{example}

We abandon the above definition and we have:
\begin{definition}
    Let $(S, \varrho)$ be a metric space with its Borel $\sigma$-algebra $B_{S}$. Let 
    $C_{b}(S) = \{ f: S \to \mathbb{R}, f\text{ is continuous and }\exists 
    \text{ constant }C s.t. \} |f(x)| \le C \quad \forall  x \in S$.

    Let $\{ \mu_n \}_{n \ge 1} \text{ and } \mu$ be Borel probability measure on $S$. Then 
    $\mu_n \longrightarrow \mu$ weakly if $\forall f \in C_{b}(S)$:
    \begin{align*}
        \int_{S}f \, d \mu_{n} \underset{n \to \infty}{\longrightarrow} \int_{S}f \, du
    \end{align*}
\end{definition}

\begin{example}
    $\int f \, d \delta_{\frac{1}{n}} = f(\frac{1}{n}) \longrightarrow f(0) = \int f d \delta_{0}$, so 
    \begin{align*}
        \delta_{\frac{1}{n}} \longrightarrow \delta_{0} \text{ weakly}.
    \end{align*}
\end{example}

Alternative notation:
\begin{align*}
    \mu_{n} \overset{w}{\mu}, \, \mu_{n} \overset{d}{\longrightarrow}, \mu_n \Longrightarrow \mu
\end{align*}
If $X_n \sim \mu_{n}, X \sim \mu$ and $\mu_n \overset{w}{\longrightarrow}\mu$ then we write 
$X_n \overset{w}{argument} X,\, X_n \overset{d}{X}, \, X_n \Longrightarrow X$.

\begin{theorem}[Portmanteam theorem]
    Let $\mu_{n}, \mu$ be Borel. Then the following are equivalent:
    \begin{enumerate}
        \item $\int f d \mu_n \longrightarrow \int f d \mu \quad \forall f \in C_{b}(S)$.
        \item $\int f d \mu_n \longrightarrow \int f d \mu$ for all bounded Lipschitz functions 
        $f: S \to \mathbb{R}$. [Def: f is Lipschitz if there exists a constant L s.t. $
        |f(x) - f(y)| \le L\varrho(x,y)$] Lipschitz functions are almost everywhere differentiable.
        \item For closed sets $F \subseteq S$, $\overline{\lim_{n \to \infty}}\mu_n(F) \le \mu(F)$.
        \item For open sets $G \subseteq S$, $\underline{\lim_{n \to \infty}} \mu_{n}G \ge \mu(G)$.
        \item If $A \in \mathcal{B}_{S}$ and $\mu(\partial A) = 0$, then $\mu_{n}(A) \longrightarrow \mu(A)$.
        \item If $f: S \to \mathbb{R}$ is a bounded Borel function, and $\mu(D_{f}) = 0$ for 
        $D_{f} = \{ x \in S: f \text{ is discontinuous at }x \}$, then 
        \begin{align*}
            \int d d \mu_{n} \longrightarrow \int f d \mu
        \end{align*}
    \end{enumerate}
\end{theorem}
\begin{remark}
    $\partial A = Bd A = \bar{A} - A^{0}$
\end{remark}


\begin{example}
    continue the example above: $\int f \, d \delta_{\frac{1}{n}} = f(\frac{1}{n}) \longrightarrow f(0) = \int f d \delta_{0}$, so 
    \begin{align*}
        \delta_{\frac{1}{n}} \longrightarrow \delta_{0} \text{ weakly}.
    \end{align*}
\end{example}

\begin{proof}
    \,

    \begin{enumerate}
        \item[(1) $\Longrightarrow$ (2)] needs no proof 
        \item[(2) $\Longrightarrow$ (3)] Distance between a point and a set  is 
        $dist(x,F) = inf \{ \varrho(x,z): z \in F \}$. 
        \begin{align*}
            f_{k}(x) = (1 - k\cdot dist(x,F))^{+}
        \end{align*}
        Here $f_{k}$ is Lipschitz.
        \begin{remark}
            Key Fact: $F \text{ closed } \Longrightarrow [x \in F \iff dist(x,F) = 0]$
        \end{remark}
        $\1_{F} \le f \le 1$ and $f_{k}(x) \searrow \1_{F}(x)$ as $k \nearrow \infty$.
        \begin{align*}
            \overline{\lim_{n \to \infty}}\mu_{n}(F) \le \overline{\lim_{n \to \infty}}
            \inf f_{k} d \mu_{n} = \int f_{k} d \mu \underset{k \to \infty}{k\longrightarrow}
            \int \1_{F} d \mu
        \end{align*}
        \item[(3) $\Longrightarrow$ (4)] by taking complements. 
        \begin{align*}
            \overline{\lim}\mu_{n}(F) \le \mu(F) \iff 
            \underline{\lim}\mu_{n}(F^{c}) \ge \mu(F^{c})
        \end{align*}
        \item[(3 $\&$ 4) $\Longrightarrow$ (5)] 
        \begin{align*}
            \overline{\lim}\mu_{n}(A) \le \overline{\lim}\mu_{n}\bar{A} \le \mu(\bar{A})\\
            \le \mu(A) + \mu(\partial A) = \mu(A)
        \end{align*}
        Now we check the other direction:
        \begin{align*}
            \underline{\lim}\mu_{n}(A) \ge \underline{\lim}\mu_{n}(A^{0}) \ge \mu(A^{0})
            \ge \mu(\bar{A}) - \mu(\partial A) \ge \mu(A)
        \end{align*}
    \end{enumerate}
\end{proof}



\end{document}